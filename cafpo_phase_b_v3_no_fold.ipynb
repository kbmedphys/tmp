{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: CAFPO Phase B v3（foldなし単発 split）\n",
    "\n",
    "## 目的\n",
    "- 既存v2（rolling fold）を、単発の train/test split に置き換える。\n",
    "- `PPO/DDPG × 3報酬` を同一 test 期間で比較する。\n",
    "- `best_method` 1条件に対して SHAP で因子寄与分析を実施する。\n",
    "\n",
    "## split 定義\n",
    "- Train: 2001-01 〜 2020-12\n",
    "- Test: 2021-01 〜 2025-12\n",
    "\n",
    "## 論文との差分（継続）\n",
    "- CRSP / 94企業特性 / FF5 は未使用（手元データ代替）。\n",
    "- 制約は Long-only（`w>=0`, `sum(w)=1`）。\n",
    "- Value-weight は時価総額データ不在のため未実装。\n",
    "- 取引コスト・スリッページは 0 固定。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "from itertools import product\n",
    "from pathlib import Path\n",
    "\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from gymnasium import spaces\n",
    "from scipy.optimize import minimize\n",
    "from stable_baselines3 import DDPG, PPO\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "PROJECT_ROOT = Path('/Users/kencharoff/workspace/projects/rl/p03_deep_reinforcement_learning_in_factor_investment')\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "OUTPUT_DIR = PROJECT_ROOT / 'outputs'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Matplotlib cache warning 回避\n",
    "MPL_DIR = OUTPUT_DIR / 'tmp_mpl'\n",
    "MPL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "os.environ['MPLCONFIGDIR'] = str(MPL_DIR)\n",
    "\n",
    "import shap  # noqa: E402\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "CONFIG = {\n",
    "    'start_date': '2001-01-31',\n",
    "    'end_date': '2025-12-31',\n",
    "    'train_start': '2001-01-31',\n",
    "    'train_end': '2020-12-31',\n",
    "    'test_start': '2021-01-31',\n",
    "    'test_end': '2025-12-31',\n",
    "    'split_name': 'train_2001_2020_test_2021_2025',\n",
    "    'num_factors': 4,\n",
    "    'hidden_dim': 32,\n",
    "    'cae_lr': 1e-3,\n",
    "    'cae_epochs': 200,\n",
    "    'lookback': 12,\n",
    "    'reward_eta': 0.01,\n",
    "    'reward_eps': 1e-8,\n",
    "    'reward_modes': ['log_return', 'diff_sharpe', 'diff_ddr'],\n",
    "    'algos': ['PPO', 'DDPG'],\n",
    "    'ppo_learning_rate': 3e-4,\n",
    "    'ppo_gamma': 0.99,\n",
    "    'ppo_n_steps': 96,\n",
    "    'ppo_batch_size': 32,\n",
    "    'ppo_ent_coef': 0.0,\n",
    "    'ppo_total_timesteps': 20_000,\n",
    "    'ddpg_learning_rate': 1e-3,\n",
    "    'ddpg_buffer_size': 50_000,\n",
    "    'ddpg_learning_starts': 1_000,\n",
    "    'ddpg_batch_size': 64,\n",
    "    'ddpg_tau': 0.005,\n",
    "    'ddpg_gamma': 0.99,\n",
    "    'ddpg_train_freq': (1, 'step'),\n",
    "    'ddpg_gradient_steps': 1,\n",
    "    'ddpg_total_timesteps': 20_000,\n",
    "    'ddpg_noise_sigma': 0.1,\n",
    "    'markowitz_window': 36,\n",
    "    'cost_bps': 0.0,\n",
    "    'shap_background': 128,\n",
    "    'shap_explain': 120,\n",
    "    'shap_kernel_nsamples': 100,\n",
    "    'perm_repeats': 10,\n",
    "    'perm_seed': 42,\n",
    "    'ig_steps': 64,\n",
    "    'ig_baseline_mode': 'train_mean',\n",
    "    'interp_methods': ['shap', 'permutation', 'integrated_gradients'],\n",
    "}\n",
    "\n",
    "print('CONFIG loaded')\n",
    "print('torch:', torch.__version__)\n",
    "print('pandas:', pd.__version__)\n",
    "print('shap:', shap.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "sector_daily = pd.read_csv(DATA_DIR / 'sector_returns.csv', parse_dates=['Date'])\nmacro_daily = pd.read_csv(DATA_DIR / 'macro.csv', parse_dates=['Date'])\nspx_daily = pd.read_csv(DATA_DIR / 'SPX.csv', parse_dates=['Date'])\n\nexpected_sector_cols = {'Date', 'XLB', 'XLE', 'XLF', 'XLI', 'XLK', 'XLP', 'XLU', 'XLV', 'XLY'}\nexpected_macro_cols = {'Date', '^VIX', 'DX-Y.NYB', 'SPRED'}\nexpected_spx_cols = {'Date', '^GSPC'}\n\nassert expected_sector_cols.issubset(sector_daily.columns), 'sector_returns.csv columns mismatch'\nassert expected_macro_cols.issubset(macro_daily.columns), 'macro.csv columns mismatch'\nassert expected_spx_cols.issubset(spx_daily.columns), 'SPX.csv columns mismatch'\n\nprint('sector_daily shape:', sector_daily.shape)\nprint('macro_daily shape:', macro_daily.shape)\nprint('spx_daily shape:', spx_daily.shape)\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "sector_daily = sector_daily.sort_values('Date').set_index('Date')\nmacro_daily = macro_daily.sort_values('Date').set_index('Date')\nspx_daily = spx_daily.sort_values('Date').set_index('Date')\n\nfull_sector_monthly = (1.0 + sector_daily).resample('ME').prod() - 1.0\nfull_macro_monthly = macro_daily.resample('ME').last()\nfull_macro_lag1 = full_macro_monthly.shift(1)\nfull_spx_monthly_ret = spx_daily.resample('ME').last().pct_change().rename(columns={'^GSPC': 'SPX_RET'})\n\ndecision_index = full_sector_monthly.index[\n    (full_sector_monthly.index >= pd.Timestamp(CONFIG['start_date']))\n    & (full_sector_monthly.index <= pd.Timestamp(CONFIG['end_date']))\n]\n\nassets = full_sector_monthly.columns.tolist()\n\nsector_monthly = full_sector_monthly.reindex(decision_index)\ntarget_monthly = full_sector_monthly.shift(-1).reindex(decision_index)\nmacro_lag1 = full_macro_lag1.reindex(decision_index).ffill().bfill()\nspx_monthly_ret = full_spx_monthly_ret.reindex(decision_index).ffill().bfill()\n\nassert sector_monthly.index.is_monotonic_increasing\nassert sector_monthly.index.is_unique\nassert target_monthly.notna().all().all(), 'target_monthly contains NaN'\n\nprint('decision months:', len(decision_index))\nprint('decision start/end:', decision_index.min().date(), decision_index.max().date())\nprint('assets:', assets)\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def rolling_prod_return(df: pd.DataFrame, window: int) -> pd.DataFrame:\n    return (1.0 + df).rolling(window=window, min_periods=1).apply(np.prod, raw=True) - 1.0\n\n\ndef downside_deviation(df: pd.DataFrame, window: int) -> pd.DataFrame:\n    down = df.clip(upper=0.0)\n    return np.sqrt((down**2).rolling(window=window, min_periods=1).mean())\n\n\ndef cs_rank_normalize(df: pd.DataFrame) -> pd.DataFrame:\n    out = np.zeros_like(df.to_numpy(dtype=np.float64), dtype=np.float64)\n    arr = df.to_numpy(dtype=np.float64)\n    for i in range(arr.shape[0]):\n        row = arr[i].copy()\n        if np.isnan(row).all():\n            out[i] = 0.0\n            continue\n        med = np.nanmedian(row)\n        row = np.where(np.isnan(row), med, row)\n        if np.allclose(row, row[0]):\n            out[i] = 0.0\n        else:\n            ranks = pd.Series(row).rank(method='average', pct=True).to_numpy(dtype=np.float64)\n            out[i] = 2.0 * ranks - 1.0\n    return pd.DataFrame(out, index=df.index, columns=df.columns)\n\n\nmom_1m = sector_monthly.copy()\nmom_3m = rolling_prod_return(sector_monthly, 3)\nmom_6m = rolling_prod_return(sector_monthly, 6)\nvol_3m = sector_monthly.rolling(window=3, min_periods=1).std().fillna(0.0)\nvol_6m = sector_monthly.rolling(window=6, min_periods=1).std().fillna(0.0)\ndownside_6m = downside_deviation(sector_monthly, 6).fillna(0.0)\n\nmacro_frames = {}\nfor raw_col, feat_name in [\n    ('^VIX', 'macro_vix_lag1'),\n    ('DX-Y.NYB', 'macro_dxy_lag1'),\n    ('SPRED', 'macro_spread_lag1'),\n]:\n    vals = macro_lag1[raw_col].to_numpy(dtype=np.float64)\n    mat = np.repeat(vals[:, None], len(assets), axis=1)\n    macro_frames[feat_name] = pd.DataFrame(mat, index=decision_index, columns=assets)\n\nraw_feature_frames = {\n    'mom_1m': mom_1m,\n    'mom_3m': mom_3m,\n    'mom_6m': mom_6m,\n    'vol_3m': vol_3m,\n    'vol_6m': vol_6m,\n    'downside_6m': downside_6m,\n    **macro_frames,\n}\n\nnorm_feature_frames = {name: cs_rank_normalize(frame) for name, frame in raw_feature_frames.items()}\n\nfeature_order = [\n    'mom_1m',\n    'mom_3m',\n    'mom_6m',\n    'vol_3m',\n    'vol_6m',\n    'downside_6m',\n    'macro_vix_lag1',\n    'macro_dxy_lag1',\n    'macro_spread_lag1',\n]\n\nfeature_tensor = np.stack(\n    [norm_feature_frames[name].to_numpy(dtype=np.float32) for name in feature_order],\n    axis=2,\n)\nreturns_tensor = sector_monthly.to_numpy(dtype=np.float32)\ntarget_tensor = target_monthly.to_numpy(dtype=np.float32)\nmacro_state = macro_lag1[['^VIX', 'DX-Y.NYB', 'SPRED']].copy().ffill().bfill().to_numpy(dtype=np.float32)\n\nassert np.isfinite(feature_tensor).all(), 'feature_tensor has non-finite values'\nassert np.isfinite(returns_tensor).all(), 'returns_tensor has non-finite values'\nassert np.isfinite(target_tensor).all(), 'target_tensor has non-finite values'\nassert np.isfinite(macro_state).all(), 'macro_state has non-finite values'\n\nnext_dates = decision_index + pd.offsets.MonthEnd(1)\nassert (next_dates > decision_index).all(), 'date alignment failed'\n\nprint('feature_tensor [T,N,P]:', feature_tensor.shape)\nprint('returns_tensor [T,N]:', returns_tensor.shape)\nprint('target_tensor [T,N]:', target_tensor.shape)\nprint('macro_state [T,3]:', macro_state.shape)\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class ConditionalAutoencoder(nn.Module):\n    def __init__(self, num_features: int, num_assets: int, hidden_dim: int, num_factors: int):\n        super().__init__()\n        self.num_assets = num_assets\n        self.num_factors = num_factors\n        self.covariates_net = nn.Sequential(\n            nn.Linear(num_features, hidden_dim),\n            nn.LeakyReLU(),\n            nn.Linear(hidden_dim, num_factors),\n        )\n        self.factor_net = nn.Linear(num_assets, num_factors)\n\n    def forward(self, z: torch.Tensor, r: torch.Tensor):\n        t, n, p = z.shape\n        beta = self.covariates_net(z.reshape(t * n, p)).reshape(t, n, self.num_factors)\n        factors = self.factor_net(r)\n        recon = torch.einsum('tnk,tk->tn', beta, factors)\n        return recon, factors\n\n    @torch.no_grad()\n    def encode_factors(self, r: torch.Tensor) -> torch.Tensor:\n        return self.factor_net(r)\n\n\ndef train_cae(z_train: np.ndarray, r_train: np.ndarray, cfg: dict):\n    set_seed(SEED)\n    model = ConditionalAutoencoder(\n        num_features=z_train.shape[2],\n        num_assets=r_train.shape[1],\n        hidden_dim=cfg['hidden_dim'],\n        num_factors=cfg['num_factors'],\n    )\n    optimizer = torch.optim.Adam(model.parameters(), lr=cfg['cae_lr'])\n    criterion = nn.MSELoss()\n\n    z_t = torch.tensor(z_train, dtype=torch.float32)\n    r_t = torch.tensor(r_train, dtype=torch.float32)\n    losses = []\n\n    model.train()\n    for _ in range(cfg['cae_epochs']):\n        optimizer.zero_grad()\n        recon, _ = model(z_t, r_t)\n        loss = criterion(recon, r_t)\n        loss.backward()\n        optimizer.step()\n        losses.append(float(loss.item()))\n\n    return model, losses\n\n\n@torch.no_grad()\ndef infer_factors(model: ConditionalAutoencoder, r_all: np.ndarray) -> np.ndarray:\n    model.eval()\n    r_t = torch.tensor(r_all, dtype=torch.float32)\n    factors = model.encode_factors(r_t)\n    return factors.cpu().numpy().astype(np.float32)\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def softmax_weights(logits: np.ndarray) -> np.ndarray:\n    logits = np.asarray(logits, dtype=np.float64)\n    logits = logits - np.max(logits)\n    exp_v = np.exp(logits)\n    den = np.sum(exp_v)\n    if den <= 0:\n        return np.ones_like(exp_v) / len(exp_v)\n    return (exp_v / den).astype(np.float64)\n\n\ndef build_state(factors: np.ndarray, macro: np.ndarray, idx: int, lookback: int) -> np.ndarray:\n    window = factors[idx - lookback + 1 : idx + 1].reshape(-1)\n    state = np.concatenate([window, macro[idx]], axis=0)\n    return state.astype(np.float32)\n\n\ndef compute_log_return_reward(port_ret: float) -> float:\n    return float(np.log1p(max(port_ret, -0.999999)))\n\n\ndef compute_diff_sharpe_reward(port_ret: float, a_prev: float, b_prev: float, eta: float, eps: float):\n    delta_a = eta * (port_ret - a_prev)\n    delta_b = eta * (port_ret * port_ret - b_prev)\n    denom = (b_prev - a_prev * a_prev + eps) ** 1.5\n    reward = (b_prev * delta_a - 0.5 * a_prev * delta_b) / denom\n    a_new = a_prev + delta_a\n    b_new = b_prev + delta_b\n    return float(reward), float(a_new), float(b_new)\n\n\ndef compute_diff_ddr_reward(port_ret: float, a_prev: float, dd2_prev: float, eta: float, eps: float):\n    dd_prev = float(np.sqrt(max(dd2_prev, eps)))\n    if port_ret > 0.0:\n        reward = (port_ret - 0.5 * a_prev) / dd_prev\n    else:\n        reward = (dd2_prev * (port_ret - 0.5 * a_prev) - 0.5 * a_prev * (port_ret**2)) / ((dd_prev**3) + eps)\n\n    a_new = a_prev + eta * (port_ret - a_prev)\n    dd2_new = dd2_prev + eta * (min(port_ret, 0.0) ** 2 - dd2_prev)\n    return float(reward), float(a_new), float(dd2_new)\n\n\nclass PortfolioEnv(gym.Env):\n    metadata = {'render_modes': []}\n\n    def __init__(\n        self,\n        factors: np.ndarray,\n        macro: np.ndarray,\n        target_returns: np.ndarray,\n        decision_dates: pd.DatetimeIndex,\n        start_idx: int,\n        end_idx: int,\n        lookback: int,\n        reward_mode: str,\n        reward_eta: float,\n        reward_eps: float,\n    ):\n        super().__init__()\n        self.factors = factors\n        self.macro = macro\n        self.target_returns = target_returns\n        self.decision_dates = decision_dates\n        self.start_idx = start_idx\n        self.end_idx = end_idx\n        self.lookback = lookback\n        self.reward_mode = reward_mode\n        self.reward_eta = reward_eta\n        self.reward_eps = reward_eps\n\n        self.n_assets = target_returns.shape[1]\n        self.obs_dim = lookback * factors.shape[1] + macro.shape[1]\n\n        self.action_space = spaces.Box(low=-10.0, high=10.0, shape=(self.n_assets,), dtype=np.float32)\n        self.observation_space = spaces.Box(\n            low=-np.inf,\n            high=np.inf,\n            shape=(self.obs_dim,),\n            dtype=np.float32,\n        )\n\n        self.ptr = None\n        self.weight_history = []\n        self.return_history = []\n\n        self.a_moment = 0.0\n        self.b_moment = 1e-6\n        self.dd2_moment = 1e-6\n\n    def reset(self, seed=None, options=None):\n        super().reset(seed=seed)\n        self.ptr = self.start_idx\n        self.weight_history = []\n        self.return_history = []\n        self.a_moment = 0.0\n        self.b_moment = 1e-6\n        self.dd2_moment = 1e-6\n        obs = build_state(self.factors, self.macro, self.ptr, self.lookback)\n        return obs, {}\n\n    def _compute_reward(self, port_ret: float) -> float:\n        if self.reward_mode == 'log_return':\n            reward = compute_log_return_reward(port_ret)\n        elif self.reward_mode == 'diff_sharpe':\n            reward, a_new, b_new = compute_diff_sharpe_reward(\n                port_ret,\n                self.a_moment,\n                self.b_moment,\n                self.reward_eta,\n                self.reward_eps,\n            )\n            self.a_moment, self.b_moment = a_new, b_new\n        elif self.reward_mode == 'diff_ddr':\n            reward, a_new, dd2_new = compute_diff_ddr_reward(\n                port_ret,\n                self.a_moment,\n                self.dd2_moment,\n                self.reward_eta,\n                self.reward_eps,\n            )\n            self.a_moment, self.dd2_moment = a_new, dd2_new\n        else:\n            raise ValueError(f'unknown reward_mode: {self.reward_mode}')\n\n        reward = float(np.nan_to_num(reward, nan=0.0, posinf=1e3, neginf=-1e3))\n        if reward > 1e3:\n            reward = 1e3\n        if reward < -1e3:\n            reward = -1e3\n        assert np.isfinite(reward), 'non-finite reward'\n        return reward\n\n    def step(self, action):\n        weights = softmax_weights(action)\n        port_ret = float(np.dot(weights, self.target_returns[self.ptr]))\n        port_ret = max(port_ret, -0.999999)\n        reward = self._compute_reward(port_ret)\n\n        self.weight_history.append(weights)\n        self.return_history.append(port_ret)\n\n        info = {\n            'date': str(self.decision_dates[self.ptr].date()),\n            'next_date': str((self.decision_dates[self.ptr] + pd.offsets.MonthEnd(1)).date()),\n            'portfolio_return': port_ret,\n            'reward_mode': self.reward_mode,\n        }\n\n        self.ptr += 1\n        terminated = self.ptr > self.end_idx\n        truncated = False\n\n        if terminated:\n            obs = np.zeros(self.obs_dim, dtype=np.float32)\n        else:\n            obs = build_state(self.factors, self.macro, self.ptr, self.lookback)\n\n        return obs, reward, terminated, truncated, info\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rl_for_fold(\n",
    "    algo_name: str,\n",
    "    reward_mode: str,\n",
    "    factors: np.ndarray,\n",
    "    macro: np.ndarray,\n",
    "    target_returns: np.ndarray,\n",
    "    decision_dates: pd.DatetimeIndex,\n",
    "    train_idx: np.ndarray,\n",
    "    cfg: dict,\n",
    "):\n",
    "    start_idx = int(train_idx[0] + cfg['lookback'] - 1)\n",
    "    end_idx = int(train_idx[-1])\n",
    "    assert start_idx <= end_idx, 'invalid train range for RL'\n",
    "\n",
    "    def make_env():\n",
    "        return PortfolioEnv(\n",
    "            factors=factors,\n",
    "            macro=macro,\n",
    "            target_returns=target_returns,\n",
    "            decision_dates=decision_dates,\n",
    "            start_idx=start_idx,\n",
    "            end_idx=end_idx,\n",
    "            lookback=cfg['lookback'],\n",
    "            reward_mode=reward_mode,\n",
    "            reward_eta=cfg['reward_eta'],\n",
    "            reward_eps=cfg['reward_eps'],\n",
    "        )\n",
    "\n",
    "    vec_env = DummyVecEnv([make_env])\n",
    "\n",
    "    if algo_name == 'PPO':\n",
    "        model = PPO(\n",
    "            policy='MlpPolicy',\n",
    "            env=vec_env,\n",
    "            learning_rate=cfg['ppo_learning_rate'],\n",
    "            gamma=cfg['ppo_gamma'],\n",
    "            n_steps=cfg['ppo_n_steps'],\n",
    "            batch_size=cfg['ppo_batch_size'],\n",
    "            ent_coef=cfg['ppo_ent_coef'],\n",
    "            seed=SEED,\n",
    "            verbose=0,\n",
    "            device='cpu',\n",
    "        )\n",
    "        model.learn(total_timesteps=cfg['ppo_total_timesteps'])\n",
    "        return model\n",
    "\n",
    "    if algo_name == 'DDPG':\n",
    "        n_actions = target_returns.shape[1]\n",
    "        noise = NormalActionNoise(\n",
    "            mean=np.zeros(n_actions, dtype=np.float32),\n",
    "            sigma=cfg['ddpg_noise_sigma'] * np.ones(n_actions, dtype=np.float32),\n",
    "        )\n",
    "        model = DDPG(\n",
    "            policy='MlpPolicy',\n",
    "            env=vec_env,\n",
    "            learning_rate=cfg['ddpg_learning_rate'],\n",
    "            buffer_size=cfg['ddpg_buffer_size'],\n",
    "            learning_starts=cfg['ddpg_learning_starts'],\n",
    "            batch_size=cfg['ddpg_batch_size'],\n",
    "            tau=cfg['ddpg_tau'],\n",
    "            gamma=cfg['ddpg_gamma'],\n",
    "            train_freq=cfg['ddpg_train_freq'],\n",
    "            gradient_steps=cfg['ddpg_gradient_steps'],\n",
    "            action_noise=noise,\n",
    "            seed=SEED,\n",
    "            verbose=0,\n",
    "            device='cpu',\n",
    "        )\n",
    "        model.learn(total_timesteps=cfg['ddpg_total_timesteps'])\n",
    "        return model\n",
    "\n",
    "    raise ValueError(f'unsupported algo: {algo_name}')\n",
    "\n",
    "\n",
    "def run_policy(\n",
    "    model,\n",
    "    method_name: str,\n",
    "    algo_name: str,\n",
    "    reward_mode: str,\n",
    "    factors: np.ndarray,\n",
    "    macro: np.ndarray,\n",
    "    target_returns: np.ndarray,\n",
    "    decision_dates: pd.DatetimeIndex,\n",
    "    test_idx: np.ndarray,\n",
    "    split_name: str,\n",
    "    lookback: int,\n",
    "):\n",
    "    rows = []\n",
    "    weights = []\n",
    "    for idx in test_idx:\n",
    "        idx = int(idx)\n",
    "        obs = build_state(factors, macro, idx, lookback)\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        w = softmax_weights(action)\n",
    "        port_ret = float(np.dot(w, target_returns[idx]))\n",
    "        port_ret = max(port_ret, -0.999999)\n",
    "        rows.append(\n",
    "            {\n",
    "                'date': decision_dates[idx],\n",
    "                'next_date': decision_dates[idx] + pd.offsets.MonthEnd(1),\n",
    "                'split': str(split_name),\n",
    "                'method': method_name,\n",
    "                'algo': algo_name,\n",
    "                'reward_mode': reward_mode,\n",
    "                'portfolio_return': port_ret,\n",
    "            }\n",
    "        )\n",
    "        weights.append(w)\n",
    "    return pd.DataFrame(rows), np.vstack(weights)\n",
    "\n",
    "\n",
    "def run_equal_weight(\n",
    "    target_returns: np.ndarray,\n",
    "    decision_dates: pd.DatetimeIndex,\n",
    "    test_idx: np.ndarray,\n",
    "    split_name: str,\n",
    "):\n",
    "    n_assets = target_returns.shape[1]\n",
    "    w = np.ones(n_assets, dtype=np.float64) / n_assets\n",
    "    rows = []\n",
    "    for idx in test_idx:\n",
    "        idx = int(idx)\n",
    "        port_ret = float(np.dot(w, target_returns[idx]))\n",
    "        port_ret = max(port_ret, -0.999999)\n",
    "        rows.append(\n",
    "            {\n",
    "                'date': decision_dates[idx],\n",
    "                'next_date': decision_dates[idx] + pd.offsets.MonthEnd(1),\n",
    "                'split': str(split_name),\n",
    "                'method': 'EqualWeight',\n",
    "                'algo': 'BASELINE',\n",
    "                'reward_mode': 'na',\n",
    "                'portfolio_return': port_ret,\n",
    "            }\n",
    "        )\n",
    "    return pd.DataFrame(rows), np.repeat(w[None, :], len(test_idx), axis=0)\n",
    "\n",
    "\n",
    "def solve_markowitz(mu: np.ndarray, cov: np.ndarray) -> np.ndarray:\n",
    "    n_assets = len(mu)\n",
    "    x0 = np.ones(n_assets, dtype=np.float64) / n_assets\n",
    "    bounds = [(0.0, 1.0)] * n_assets\n",
    "    constraints = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1.0},)\n",
    "\n",
    "    def neg_sharpe(w):\n",
    "        ret = float(np.dot(w, mu))\n",
    "        vol = float(np.sqrt(max(np.dot(w, cov @ w), 1e-12)))\n",
    "        return -(ret / vol)\n",
    "\n",
    "    res = minimize(neg_sharpe, x0=x0, method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "    if not res.success:\n",
    "        return x0\n",
    "    w = np.clip(res.x, 0.0, 1.0)\n",
    "    s = w.sum()\n",
    "    return x0 if s <= 0 else (w / s)\n",
    "\n",
    "\n",
    "def run_markowitz_historical(\n",
    "    returns_hist: np.ndarray,\n",
    "    target_returns: np.ndarray,\n",
    "    decision_dates: pd.DatetimeIndex,\n",
    "    test_idx: np.ndarray,\n",
    "    split_name: str,\n",
    "    window: int,\n",
    "):\n",
    "    rows = []\n",
    "    weights = []\n",
    "    n_assets = returns_hist.shape[1]\n",
    "    eq = np.ones(n_assets, dtype=np.float64) / n_assets\n",
    "\n",
    "    for idx in test_idx:\n",
    "        idx = int(idx)\n",
    "        start = max(0, idx - window + 1)\n",
    "        hist = returns_hist[start : idx + 1]\n",
    "        if hist.shape[0] < 2:\n",
    "            w = eq.copy()\n",
    "        else:\n",
    "            mu = np.nanmean(hist, axis=0)\n",
    "            cov = np.cov(hist, rowvar=False)\n",
    "            cov = np.nan_to_num(cov, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            cov = cov + np.eye(cov.shape[0]) * 1e-6\n",
    "            w = solve_markowitz(mu, cov)\n",
    "\n",
    "        port_ret = float(np.dot(w, target_returns[idx]))\n",
    "        port_ret = max(port_ret, -0.999999)\n",
    "        rows.append(\n",
    "            {\n",
    "                'date': decision_dates[idx],\n",
    "                'next_date': decision_dates[idx] + pd.offsets.MonthEnd(1),\n",
    "                'split': str(split_name),\n",
    "                'method': 'MarkowitzHist',\n",
    "                'algo': 'BASELINE',\n",
    "                'reward_mode': 'na',\n",
    "                'portfolio_return': port_ret,\n",
    "            }\n",
    "        )\n",
    "        weights.append(w)\n",
    "\n",
    "    return pd.DataFrame(rows), np.vstack(weights)\n",
    "\n",
    "\n",
    "def compute_metrics(ret: np.ndarray) -> dict:\n",
    "    ret = np.asarray(ret, dtype=np.float64)\n",
    "    compound = float(np.prod(1.0 + ret) - 1.0)\n",
    "    mean_m = float(np.mean(ret))\n",
    "    std_m = float(np.std(ret, ddof=1)) if len(ret) > 1 else 0.0\n",
    "    sharpe = 0.0 if std_m == 0.0 else float(np.sqrt(12.0) * mean_m / std_m)\n",
    "    wealth = np.cumprod(1.0 + ret)\n",
    "    run_max = np.maximum.accumulate(wealth)\n",
    "    drawdown = wealth / np.maximum(run_max, 1e-12) - 1.0\n",
    "    max_dd = float(abs(np.min(drawdown)))\n",
    "    sterling = 0.0 if max_dd == 0.0 else float(mean_m / max_dd)\n",
    "    return {\n",
    "        'CompoundReturn': compound,\n",
    "        'SharpeRatio': sharpe,\n",
    "        'SterlingRatio': sterling,\n",
    "    }\n",
    "\n",
    "\n",
    "def assert_long_only(weights: np.ndarray, atol: float = 1e-8):\n",
    "    assert np.all(weights >= -atol), 'weights have negative values'\n",
    "    assert np.allclose(weights.sum(axis=1), 1.0, atol=atol), 'weights do not sum to 1'\n",
    "\n",
    "\n",
    "class PPOActionWrapper(nn.Module):\n",
    "    def __init__(self, policy):\n",
    "        super().__init__()\n",
    "        self.policy = policy\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.ndim == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        x = x.float()\n",
    "        features = self.policy.extract_features(x)\n",
    "        latent_pi, _ = self.policy.mlp_extractor(features)\n",
    "        return self.policy.action_net(latent_pi)\n",
    "\n",
    "\n",
    "class DDPGActionWrapper(nn.Module):\n",
    "    def __init__(self, actor):\n",
    "        super().__init__()\n",
    "        self.actor = actor\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.ndim == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        return self.actor(x.float())\n",
    "\n",
    "\n",
    "def build_action_module(model, algo_name: str):\n",
    "    if algo_name == 'PPO':\n",
    "        module = PPOActionWrapper(model.policy)\n",
    "    elif algo_name == 'DDPG':\n",
    "        module = DDPGActionWrapper(model.actor)\n",
    "    else:\n",
    "        raise ValueError(f'unknown algo_name: {algo_name}')\n",
    "    module.eval()\n",
    "    return module\n",
    "\n",
    "\n",
    "def collect_states(factors: np.ndarray, macro: np.ndarray, idx_array: np.ndarray, lookback: int) -> np.ndarray:\n",
    "    valid_idx = [int(i) for i in idx_array if int(i) >= lookback - 1]\n",
    "    states = [build_state(factors, macro, i, lookback) for i in valid_idx]\n",
    "    return np.vstack(states) if states else np.zeros((0, lookback * factors.shape[1] + macro.shape[1]), dtype=np.float32)\n",
    "\n",
    "\n",
    "def standardize_shap_values(shap_values, n_samples: int, n_features: int) -> np.ndarray:\n",
    "    if isinstance(shap_values, list):\n",
    "        mats = []\n",
    "        for v in shap_values:\n",
    "            arr = np.asarray(v)\n",
    "            arr = np.squeeze(arr)\n",
    "            if arr.ndim == 1:\n",
    "                arr = arr.reshape(1, -1)\n",
    "            if arr.shape != (n_samples, n_features):\n",
    "                arr = arr.reshape(n_samples, n_features)\n",
    "            mats.append(arr)\n",
    "        return np.stack(mats, axis=-1)\n",
    "\n",
    "    arr = np.asarray(shap_values)\n",
    "    arr = np.squeeze(arr)\n",
    "    if arr.ndim == 2:\n",
    "        if arr.shape == (n_samples, n_features):\n",
    "            return arr[:, :, None]\n",
    "        if arr.shape == (n_features, n_samples):\n",
    "            return arr.T[:, :, None]\n",
    "    if arr.ndim == 3:\n",
    "        if arr.shape[0] == n_samples and arr.shape[1] == n_features:\n",
    "            return arr\n",
    "        if arr.shape[1] == n_samples and arr.shape[2] == n_features:\n",
    "            return np.transpose(arr, (1, 2, 0))\n",
    "        if arr.shape[0] == n_features and arr.shape[1] == n_samples:\n",
    "            return np.transpose(arr, (1, 0, 2))\n",
    "    raise ValueError(f'unexpected shap value shape: {np.shape(shap_values)}')\n",
    "\n",
    "\n",
    "def make_state_feature_names(lookback: int, num_factors: int):\n",
    "    names = []\n",
    "    for lag in range(lookback):\n",
    "        lag_label = lookback - lag\n",
    "        for k in range(num_factors):\n",
    "            names.append(f'factor{k+1}_lag{lag_label}')\n",
    "    names.extend(['macro_vix_lag1', 'macro_dxy_lag1', 'macro_spread_lag1'])\n",
    "    return names\n",
    "\n",
    "\n",
    "def normalize_importance_series(values: pd.Series) -> pd.Series:\n",
    "    total = float(values.sum())\n",
    "    if np.isfinite(total) and total > 0.0:\n",
    "        return values / total\n",
    "    # 情報量がゼロの場合は等配分で正規化し、比較可能な形を維持する\n",
    "    n = len(values)\n",
    "    if n == 0:\n",
    "        return values\n",
    "    return pd.Series(np.ones(n, dtype=np.float64) / float(n), index=values.index)\n",
    "\n",
    "\n",
    "def aggregate_factor_contrib_from_shap(feature_importance: np.ndarray, lookback: int, num_factors: int) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for k in range(num_factors):\n",
    "        idxs = [lag * num_factors + k for lag in range(lookback)]\n",
    "        val = float(np.sum(feature_importance[idxs]))\n",
    "        rows.append({'factor': f'factor{k+1}', 'importance': val})\n",
    "    out = pd.DataFrame(rows)\n",
    "    out['normalized_importance'] = normalize_importance_series(out['importance'])\n",
    "    return out\n",
    "\n",
    "\n",
    "def aggregate_factor_contrib_from_feature_importance(feature_importance: np.ndarray, lookback: int, num_factors: int) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for k in range(num_factors):\n",
    "        idxs = [lag * num_factors + k for lag in range(lookback)]\n",
    "        val = float(np.sum(feature_importance[idxs]))\n",
    "        rows.append({'factor': f'factor{k+1}', 'importance': val})\n",
    "    out = pd.DataFrame(rows)\n",
    "    out['normalized_importance'] = normalize_importance_series(out['importance'])\n",
    "    return out\n",
    "\n",
    "\n",
    "def compute_policy_portfolio_returns(model, algo_name: str, states: np.ndarray, next_returns: np.ndarray):\n",
    "    assert states.shape[0] == next_returns.shape[0], 'states and next_returns length mismatch'\n",
    "    action_module = build_action_module(model, algo_name)\n",
    "    with torch.no_grad():\n",
    "        action = action_module(torch.tensor(states, dtype=torch.float32)).cpu().numpy()\n",
    "    if action.ndim == 1:\n",
    "        action = action.reshape(1, -1)\n",
    "\n",
    "    weights = np.vstack([softmax_weights(a) for a in action])\n",
    "    port = np.einsum('ij,ij->i', weights, next_returns.astype(np.float64))\n",
    "    port = np.clip(port, -0.999999, None)\n",
    "    return float(np.mean(port)), port.astype(np.float64), weights.astype(np.float64)\n",
    "\n",
    "\n",
    "def plot_factor_contrib(factor_df: pd.DataFrame, title: str, out_path: Path):\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax.bar(factor_df['factor'], factor_df['normalized_importance'])\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel('Normalized Importance')\n",
    "    ax.set_xlabel('Factor')\n",
    "    ax.grid(True, axis='y', alpha=0.3)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_path, dpi=150)\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "trained_models = {}\n",
    "split_name = CONFIG['split_name']\n",
    "\n",
    "train_mask = (decision_index >= pd.Timestamp(CONFIG['train_start'])) & (decision_index <= pd.Timestamp(CONFIG['train_end']))\n",
    "test_mask = (decision_index >= pd.Timestamp(CONFIG['test_start'])) & (decision_index <= pd.Timestamp(CONFIG['test_end']))\n",
    "\n",
    "train_idx = np.where(train_mask)[0]\n",
    "test_idx = np.where(test_mask)[0]\n",
    "\n",
    "assert len(train_idx) == 240, f'train months != 240: {len(train_idx)}'\n",
    "assert len(test_idx) == 60, f'test months != 60: {len(test_idx)}'\n",
    "assert len(np.intersect1d(train_idx, test_idx)) == 0, 'train/test overlap detected'\n",
    "\n",
    "train_dates = decision_index[train_idx]\n",
    "test_dates = decision_index[test_idx]\n",
    "assert train_dates.is_monotonic_increasing and train_dates.is_unique\n",
    "assert test_dates.is_monotonic_increasing and test_dates.is_unique\n",
    "assert train_dates.max() < test_dates.min(), 'train end must be earlier than test start'\n",
    "\n",
    "z_train = feature_tensor[train_idx]\n",
    "r_train = returns_tensor[train_idx]\n",
    "cae_model, cae_losses = train_cae(z_train, r_train, CONFIG)\n",
    "factors_all = infer_factors(cae_model, returns_tensor)\n",
    "\n",
    "# t+1 報酬整合のため、trainの最終月はRL学習から除外\n",
    "rl_train_idx = train_idx[:-1]\n",
    "assert rl_train_idx.max() < test_idx.min(), 'RL train should end before test starts'\n",
    "\n",
    "for algo_name, reward_mode in product(CONFIG['algos'], CONFIG['reward_modes']):\n",
    "    method_name = f'{algo_name}_{reward_mode}'\n",
    "    rl_model = train_rl_for_fold(\n",
    "        algo_name=algo_name,\n",
    "        reward_mode=reward_mode,\n",
    "        factors=factors_all,\n",
    "        macro=macro_state,\n",
    "        target_returns=target_tensor,\n",
    "        decision_dates=decision_index,\n",
    "        train_idx=rl_train_idx,\n",
    "        cfg=CONFIG,\n",
    "    )\n",
    "\n",
    "    rl_df, rl_w = run_policy(\n",
    "        model=rl_model,\n",
    "        method_name=method_name,\n",
    "        algo_name=algo_name,\n",
    "        reward_mode=reward_mode,\n",
    "        factors=factors_all,\n",
    "        macro=macro_state,\n",
    "        target_returns=target_tensor,\n",
    "        decision_dates=decision_index,\n",
    "        test_idx=test_idx,\n",
    "        split_name=split_name,\n",
    "        lookback=CONFIG['lookback'],\n",
    "    )\n",
    "    assert_long_only(rl_w)\n",
    "    all_results.append(rl_df)\n",
    "\n",
    "    trained_models[(algo_name, reward_mode)] = {\n",
    "        'model': rl_model,\n",
    "        'factors_all': factors_all.copy(),\n",
    "        'train_idx': rl_train_idx.copy(),\n",
    "        'test_idx': test_idx.copy(),\n",
    "    }\n",
    "\n",
    "eq_df, eq_w = run_equal_weight(\n",
    "    target_returns=target_tensor,\n",
    "    decision_dates=decision_index,\n",
    "    test_idx=test_idx,\n",
    "    split_name=split_name,\n",
    ")\n",
    "mk_df, mk_w = run_markowitz_historical(\n",
    "    returns_hist=returns_tensor,\n",
    "    target_returns=target_tensor,\n",
    "    decision_dates=decision_index,\n",
    "    test_idx=test_idx,\n",
    "    split_name=split_name,\n",
    "    window=CONFIG['markowitz_window'],\n",
    ")\n",
    "assert_long_only(eq_w)\n",
    "assert_long_only(mk_w)\n",
    "\n",
    "all_results.extend([eq_df, mk_df])\n",
    "\n",
    "results_df = pd.concat(all_results, ignore_index=True)\n",
    "results_df = results_df.sort_values(['date', 'method']).reset_index(drop=True)\n",
    "\n",
    "assert np.isfinite(results_df['portfolio_return'].to_numpy(dtype=np.float64)).all(), 'non-finite return found'\n",
    "assert results_df['split'].nunique() == 1 and results_df['split'].iloc[0] == split_name\n",
    "\n",
    "expected_points = len(test_idx)\n",
    "counts = results_df.groupby('method')['date'].nunique().to_dict()\n",
    "for method_name, n_points in counts.items():\n",
    "    assert n_points == expected_points, f'{method_name} points mismatch: {n_points}'\n",
    "\n",
    "# 6条件（algo x reward）の網羅確認\n",
    "rl_grid = results_df[results_df['algo'] != 'BASELINE'].groupby(['algo', 'reward_mode'])['date'].nunique().reset_index(name='n_dates')\n",
    "assert len(rl_grid) == 6, 'RL condition grid is incomplete'\n",
    "assert (rl_grid['n_dates'] == expected_points).all(), 'some RL conditions miss dates'\n",
    "\n",
    "method_presence = results_df.pivot_table(index='date', columns='method', values='portfolio_return', aggfunc='size', fill_value=0)\n",
    "assert (method_presence > 0).all().all(), 'methods are not aligned on identical test dates'\n",
    "\n",
    "print('split_name:', split_name)\n",
    "print('train months:', len(train_idx), train_dates.min().date(), train_dates.max().date())\n",
    "print('test months:', len(test_idx), test_dates.min().date(), test_dates.max().date())\n",
    "print('results rows:', len(results_df))\n",
    "print('methods:', sorted(results_df['method'].unique()))\n",
    "print(f'CAE loss(last)={cae_losses[-1]:.6f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_records = []\n",
    "for (method_name, algo_name, reward_mode), g in results_df.groupby(['method', 'algo', 'reward_mode']):\n",
    "    g = g.sort_values('date')\n",
    "    m = compute_metrics(g['portfolio_return'].to_numpy(dtype=np.float64))\n",
    "    m.update({'method': method_name, 'algo': algo_name, 'reward_mode': reward_mode})\n",
    "    metrics_records.append(m)\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_records)[\n",
    "    ['method', 'algo', 'reward_mode', 'CompoundReturn', 'SharpeRatio', 'SterlingRatio']\n",
    "].sort_values(['algo', 'reward_mode', 'method']).reset_index(drop=True)\n",
    "\n",
    "split_records = []\n",
    "for (year, method_name, algo_name, reward_mode), g in results_df.assign(year=results_df['date'].dt.year).groupby(\n",
    "    ['year', 'method', 'algo', 'reward_mode']\n",
    "):\n",
    "    m = compute_metrics(g.sort_values('date')['portfolio_return'].to_numpy(dtype=np.float64))\n",
    "    m.update({'year': int(year), 'method': method_name, 'algo': algo_name, 'reward_mode': reward_mode})\n",
    "    split_records.append(m)\n",
    "\n",
    "split_grid_df = pd.DataFrame(split_records)[\n",
    "    ['year', 'method', 'algo', 'reward_mode', 'CompoundReturn', 'SharpeRatio', 'SterlingRatio']\n",
    "].sort_values(['year', 'algo', 'reward_mode', 'method']).reset_index(drop=True)\n",
    "\n",
    "wide_returns = (\n",
    "    results_df.pivot(index='date', columns='method', values='portfolio_return')\n",
    "    .sort_index()\n",
    "    .astype(float)\n",
    ")\n",
    "cumulative = (1.0 + wide_returns).cumprod() - 1.0\n",
    "\n",
    "metrics_path = OUTPUT_DIR / 'phase_b_v3_metrics.csv'\n",
    "cum_csv_path = OUTPUT_DIR / 'phase_b_v3_cumulative_returns.csv'\n",
    "cum_png_path = OUTPUT_DIR / 'phase_b_v3_cumulative_returns.png'\n",
    "split_grid_path = OUTPUT_DIR / 'phase_b_v3_split_grid.csv'\n",
    "\n",
    "metrics_df.to_csv(metrics_path, index=False)\n",
    "cumulative.reset_index().to_csv(cum_csv_path, index=False)\n",
    "split_grid_df.to_csv(split_grid_path, index=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13, 7))\n",
    "for col in cumulative.columns:\n",
    "    ax.plot(cumulative.index, cumulative[col], label=col)\n",
    "ax.set_title('Phase B v3 Cumulative Returns (single split: 2001-2020 train / 2021-2025 test)')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Cumulative Return')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(loc='upper left', ncol=2, fontsize=8)\n",
    "fig.tight_layout()\n",
    "fig.savefig(cum_png_path, dpi=150)\n",
    "plt.close(fig)\n",
    "\n",
    "rl_metrics = metrics_df[metrics_df['algo'].isin(['PPO', 'DDPG'])].copy()\n",
    "best_row = rl_metrics.sort_values(['SharpeRatio', 'CompoundReturn'], ascending=False).iloc[0]\n",
    "best_method = str(best_row['method'])\n",
    "best_algo = str(best_row['algo'])\n",
    "best_reward = str(best_row['reward_mode'])\n",
    "\n",
    "print('best_method:', best_method)\n",
    "print('best_algo:', best_algo, 'best_reward:', best_reward)\n",
    "print('saved:', metrics_path)\n",
    "print('saved:', cum_csv_path)\n",
    "print('saved:', cum_png_path)\n",
    "print('saved:', split_grid_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_shap_for_best_model(best_artifact: dict, algo_name: str, cfg: dict):\n",
    "    model = best_artifact['model']\n",
    "    factors_all = best_artifact['factors_all']\n",
    "    train_idx = best_artifact['train_idx']\n",
    "    test_idx = best_artifact['test_idx']\n",
    "\n",
    "    train_states = collect_states(factors_all, macro_state, train_idx, cfg['lookback'])\n",
    "    test_states = collect_states(factors_all, macro_state, test_idx, cfg['lookback'])\n",
    "    test_returns = target_tensor[test_idx].astype(np.float32)\n",
    "    assert len(train_states) > 0 and len(test_states) > 0, 'insufficient states for SHAP'\n",
    "    assert test_states.shape[0] == test_returns.shape[0], 'SHAP test states/returns mismatch'\n",
    "\n",
    "    rng = np.random.default_rng(SEED)\n",
    "    bg_n = min(cfg['shap_background'], len(train_states))\n",
    "    ex_n = min(cfg['shap_explain'], len(test_states))\n",
    "    bg_idx = rng.choice(len(train_states), size=bg_n, replace=False)\n",
    "    ex_idx = rng.choice(len(test_states), size=ex_n, replace=False)\n",
    "    bg_np = train_states[bg_idx].astype(np.float32)\n",
    "    ex_np = test_states[ex_idx].astype(np.float32)\n",
    "\n",
    "    action_module = build_action_module(model, algo_name)\n",
    "\n",
    "    shap_method = 'GradientExplainer'\n",
    "    try:\n",
    "        bg_t = torch.tensor(bg_np, dtype=torch.float32)\n",
    "        ex_t = torch.tensor(ex_np, dtype=torch.float32)\n",
    "        explainer = shap.GradientExplainer(action_module, bg_t)\n",
    "        shap_values = explainer.shap_values(ex_t)\n",
    "        shap_arr = standardize_shap_values(shap_values, n_samples=ex_np.shape[0], n_features=ex_np.shape[1])\n",
    "    except Exception as e:\n",
    "        shap_method = f'KernelExplainerFallback:{type(e).__name__}'\n",
    "        bg_small = bg_np[: min(20, len(bg_np))]\n",
    "        ex_small = ex_np[: min(20, len(ex_np))]\n",
    "\n",
    "        def predict_fn(x: np.ndarray) -> np.ndarray:\n",
    "            x_t = torch.tensor(x, dtype=torch.float32)\n",
    "            with torch.no_grad():\n",
    "                out = action_module(x_t).cpu().numpy()\n",
    "            return out\n",
    "\n",
    "        explainer = shap.KernelExplainer(predict_fn, bg_small)\n",
    "        shap_values = explainer.shap_values(ex_small, nsamples=cfg['shap_kernel_nsamples'])\n",
    "        shap_arr = standardize_shap_values(shap_values, n_samples=ex_small.shape[0], n_features=ex_small.shape[1])\n",
    "        ex_np = ex_small\n",
    "\n",
    "    abs_sum_actions = np.abs(shap_arr).sum(axis=-1)  # [samples, features]\n",
    "    feature_importance = np.nan_to_num(abs_sum_actions.mean(axis=0), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    # SHAPが全ゼロ/非有限になる場合は、代替として勾配サリエンシーを使用\n",
    "    if (not np.isfinite(feature_importance).all()) or float(np.sum(feature_importance)) <= 0.0:\n",
    "        x_t = torch.tensor(ex_np, dtype=torch.float32, requires_grad=True)\n",
    "        action_module.zero_grad(set_to_none=True)\n",
    "        out = action_module(x_t)\n",
    "        objective = out.abs().sum()\n",
    "        objective.backward()\n",
    "        grad = x_t.grad.detach().cpu().numpy()\n",
    "        feature_importance = np.nan_to_num(np.abs(grad).mean(axis=0), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        shap_method = shap_method + '+GradientSaliencyFallback'\n",
    "\n",
    "    feature_names = make_state_feature_names(cfg['lookback'], cfg['num_factors'])\n",
    "    feat_df = pd.DataFrame(\n",
    "        {\n",
    "            'feature': feature_names,\n",
    "            'importance': feature_importance,\n",
    "        }\n",
    "    )\n",
    "    feat_df['normalized_importance'] = normalize_importance_series(feat_df['importance'])\n",
    "    feat_df = feat_df.sort_values('importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    factor_df = aggregate_factor_contrib_from_feature_importance(\n",
    "        feature_importance=feature_importance,\n",
    "        lookback=cfg['lookback'],\n",
    "        num_factors=cfg['num_factors'],\n",
    "    )\n",
    "    factor_df = factor_df.sort_values('importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return {\n",
    "        'feature_df': feat_df,\n",
    "        'factor_df': factor_df,\n",
    "        'feature_importance': feature_importance,\n",
    "        'shap_method': shap_method,\n",
    "        'n_background': int(bg_np.shape[0]),\n",
    "        'n_explain': int(ex_np.shape[0]),\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_permutation_importance_for_best_model(best_artifact: dict, algo_name: str, cfg: dict):\n",
    "    model = best_artifact['model']\n",
    "    factors_all = best_artifact['factors_all']\n",
    "    test_idx = best_artifact['test_idx']\n",
    "\n",
    "    test_states = collect_states(factors_all, macro_state, test_idx, cfg['lookback'])\n",
    "    test_returns = target_tensor[test_idx].astype(np.float32)\n",
    "    assert test_states.shape[0] == test_returns.shape[0], 'Permutation test states/returns mismatch'\n",
    "\n",
    "    baseline_mean_return, _, _ = compute_policy_portfolio_returns(\n",
    "        model=model,\n",
    "        algo_name=algo_name,\n",
    "        states=test_states,\n",
    "        next_returns=test_returns,\n",
    "    )\n",
    "\n",
    "    rng = np.random.default_rng(cfg['perm_seed'])\n",
    "    n_samples, n_features = test_states.shape\n",
    "    feature_names = make_state_feature_names(cfg['lookback'], cfg['num_factors'])\n",
    "\n",
    "    rows = []\n",
    "    importance = np.zeros(n_features, dtype=np.float64)\n",
    "\n",
    "    for j in range(n_features):\n",
    "        drops = []\n",
    "        for _ in range(int(cfg['perm_repeats'])):\n",
    "            perm_states = test_states.copy()\n",
    "            perm_idx = rng.permutation(n_samples)\n",
    "            perm_states[:, j] = perm_states[perm_idx, j]\n",
    "            perm_mean_return, _, _ = compute_policy_portfolio_returns(\n",
    "                model=model,\n",
    "                algo_name=algo_name,\n",
    "                states=perm_states,\n",
    "                next_returns=test_returns,\n",
    "            )\n",
    "            drop = float(baseline_mean_return - perm_mean_return)\n",
    "            if not np.isfinite(drop):\n",
    "                drop = 0.0\n",
    "            drops.append(drop)\n",
    "\n",
    "        mean_drop = float(np.mean(drops))\n",
    "        std_drop = float(np.std(drops, ddof=1)) if len(drops) > 1 else 0.0\n",
    "        imp = float(max(mean_drop, 0.0))\n",
    "        importance[j] = imp\n",
    "        rows.append(\n",
    "            {\n",
    "                'feature': feature_names[j],\n",
    "                'mean_drop': mean_drop,\n",
    "                'std_drop': std_drop,\n",
    "                'importance': imp,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    feat_df = pd.DataFrame(rows)\n",
    "    feat_df['normalized_importance'] = normalize_importance_series(feat_df['importance'])\n",
    "    feat_df = feat_df.sort_values('importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    factor_df = aggregate_factor_contrib_from_feature_importance(\n",
    "        feature_importance=importance,\n",
    "        lookback=cfg['lookback'],\n",
    "        num_factors=cfg['num_factors'],\n",
    "    )\n",
    "    factor_df = factor_df.sort_values('importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return {\n",
    "        'feature_df': feat_df,\n",
    "        'factor_df': factor_df,\n",
    "        'baseline_mean_return': float(baseline_mean_return),\n",
    "        'perm_repeats': int(cfg['perm_repeats']),\n",
    "        'n_samples': int(n_samples),\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_integrated_gradients_for_best_model(best_artifact: dict, algo_name: str, cfg: dict):\n",
    "    model = best_artifact['model']\n",
    "    factors_all = best_artifact['factors_all']\n",
    "    train_idx = best_artifact['train_idx']\n",
    "    test_idx = best_artifact['test_idx']\n",
    "\n",
    "    train_states = collect_states(factors_all, macro_state, train_idx, cfg['lookback'])\n",
    "    test_states = collect_states(factors_all, macro_state, test_idx, cfg['lookback'])\n",
    "    test_returns = target_tensor[test_idx].astype(np.float32)\n",
    "    assert train_states.shape[0] > 0 and test_states.shape[0] > 0, 'IG requires non-empty states'\n",
    "    assert test_states.shape[0] == test_returns.shape[0], 'IG test states/returns mismatch'\n",
    "\n",
    "    if cfg['ig_baseline_mode'] == 'train_mean':\n",
    "        baseline_vec = train_states.mean(axis=0).astype(np.float32)\n",
    "    else:\n",
    "        baseline_vec = np.zeros(test_states.shape[1], dtype=np.float32)\n",
    "\n",
    "    steps = int(cfg['ig_steps'])\n",
    "    action_module = build_action_module(model, algo_name)\n",
    "\n",
    "    baseline_t = torch.tensor(baseline_vec, dtype=torch.float32)\n",
    "    feature_attr = []\n",
    "\n",
    "    for state_vec, ret_vec in zip(test_states, test_returns):\n",
    "        x_t = torch.tensor(state_vec, dtype=torch.float32)\n",
    "        r_t = torch.tensor(ret_vec, dtype=torch.float32)\n",
    "        total_grad = torch.zeros_like(x_t)\n",
    "\n",
    "        for alpha in np.linspace(1.0 / steps, 1.0, steps):\n",
    "            interp = baseline_t + float(alpha) * (x_t - baseline_t)\n",
    "            interp = interp.detach().clone().requires_grad_(True)\n",
    "            action = action_module(interp.unsqueeze(0))[0]\n",
    "            weight = torch.softmax(action, dim=-1)\n",
    "            target_scalar = torch.sum(weight * r_t)\n",
    "            grad = torch.autograd.grad(target_scalar, interp, retain_graph=False, create_graph=False)[0]\n",
    "            grad = torch.nan_to_num(grad, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            total_grad += grad\n",
    "\n",
    "        avg_grad = total_grad / float(steps)\n",
    "        ig = (x_t - baseline_t) * avg_grad\n",
    "        feature_attr.append(np.abs(ig.detach().cpu().numpy()))\n",
    "\n",
    "    attr_arr = np.vstack(feature_attr)\n",
    "    feature_importance = np.nan_to_num(attr_arr.mean(axis=0), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    feature_names = make_state_feature_names(cfg['lookback'], cfg['num_factors'])\n",
    "    feat_df = pd.DataFrame(\n",
    "        {\n",
    "            'feature': feature_names,\n",
    "            'importance': feature_importance,\n",
    "        }\n",
    "    )\n",
    "    feat_df['normalized_importance'] = normalize_importance_series(feat_df['importance'])\n",
    "    feat_df = feat_df.sort_values('importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    factor_df = aggregate_factor_contrib_from_feature_importance(\n",
    "        feature_importance=feature_importance,\n",
    "        lookback=cfg['lookback'],\n",
    "        num_factors=cfg['num_factors'],\n",
    "    )\n",
    "    factor_df = factor_df.sort_values('importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return {\n",
    "        'feature_df': feat_df,\n",
    "        'factor_df': factor_df,\n",
    "        'ig_steps': steps,\n",
    "        'baseline_mode': cfg['ig_baseline_mode'],\n",
    "        'n_samples': int(test_states.shape[0]),\n",
    "    }\n",
    "\n",
    "\n",
    "best_key = (best_algo, best_reward)\n",
    "assert best_key in trained_models, f'best model not found in trained model store: {best_key}'\n",
    "best_artifact = trained_models[best_key]\n",
    "\n",
    "shap_out = compute_shap_for_best_model(best_artifact, best_algo, CONFIG)\n",
    "perm_out = compute_permutation_importance_for_best_model(best_artifact, best_algo, CONFIG)\n",
    "ig_out = compute_integrated_gradients_for_best_model(best_artifact, best_algo, CONFIG)\n",
    "\n",
    "shap_feature_path = OUTPUT_DIR / 'phase_b_v3_shap_feature_importance.csv'\n",
    "shap_factor_path = OUTPUT_DIR / 'phase_b_v3_shap_factor_contrib.csv'\n",
    "shap_factor_png_path = OUTPUT_DIR / 'phase_b_v3_shap_factor_contrib.png'\n",
    "\n",
    "perm_feature_path = OUTPUT_DIR / 'phase_b_v3_perm_feature_importance.csv'\n",
    "perm_factor_path = OUTPUT_DIR / 'phase_b_v3_perm_factor_contrib.csv'\n",
    "perm_factor_png_path = OUTPUT_DIR / 'phase_b_v3_perm_factor_contrib.png'\n",
    "\n",
    "ig_feature_path = OUTPUT_DIR / 'phase_b_v3_ig_feature_importance.csv'\n",
    "ig_factor_path = OUTPUT_DIR / 'phase_b_v3_ig_factor_contrib.csv'\n",
    "ig_factor_png_path = OUTPUT_DIR / 'phase_b_v3_ig_factor_contrib.png'\n",
    "\n",
    "factor_compare_path = OUTPUT_DIR / 'phase_b_v3_interpretability_factor_compare.csv'\n",
    "evidence_path = OUTPUT_DIR / 'phase_b_v3_experiment_evidence.md'\n",
    "\n",
    "shap_out['feature_df'].to_csv(shap_feature_path, index=False)\n",
    "shap_out['factor_df'].to_csv(shap_factor_path, index=False)\n",
    "perm_out['feature_df'].to_csv(perm_feature_path, index=False)\n",
    "perm_out['factor_df'].to_csv(perm_factor_path, index=False)\n",
    "ig_out['feature_df'].to_csv(ig_feature_path, index=False)\n",
    "ig_out['factor_df'].to_csv(ig_factor_path, index=False)\n",
    "\n",
    "plot_factor_contrib(shap_out['factor_df'], f'SHAP Factor Contribution ({best_method})', shap_factor_png_path)\n",
    "plot_factor_contrib(perm_out['factor_df'], f'Permutation Factor Contribution ({best_method})', perm_factor_png_path)\n",
    "plot_factor_contrib(ig_out['factor_df'], f'IG Factor Contribution ({best_method})', ig_factor_png_path)\n",
    "\n",
    "factor_compare_df = pd.concat(\n",
    "    [\n",
    "        shap_out['factor_df'].assign(method='SHAP'),\n",
    "        perm_out['factor_df'].assign(method='Permutation'),\n",
    "        ig_out['factor_df'].assign(method='IntegratedGradients'),\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ")\n",
    "factor_compare_df = factor_compare_df[['method', 'factor', 'importance', 'normalized_importance']]\n",
    "factor_compare_df.to_csv(factor_compare_path, index=False)\n",
    "\n",
    "# テスト結果（要件対応）\n",
    "state_dim_expected = CONFIG['lookback'] * CONFIG['num_factors'] + 3\n",
    "shape_alignment_pass = bool(\n",
    "    len(shap_out['feature_df']) == state_dim_expected\n",
    "    and len(perm_out['feature_df']) == state_dim_expected\n",
    "    and len(ig_out['feature_df']) == state_dim_expected\n",
    "    and best_artifact['test_idx'].shape[0] == perm_out['n_samples']\n",
    ")\n",
    "\n",
    "data_integrity_pass = bool(\n",
    "    decision_index.is_monotonic_increasing\n",
    "    and decision_index.is_unique\n",
    "    and len(np.intersect1d(train_idx, test_idx)) == 0\n",
    "    and decision_index[train_idx].max() < decision_index[test_idx].min()\n",
    ")\n",
    "alignment_pass = bool(train_idx.max() < test_idx.min())\n",
    "reward_finite_pass = bool(np.isfinite(results_df['portfolio_return'].to_numpy(dtype=np.float64)).all())\n",
    "coverage_pass = bool(len(rl_grid) == 6 and (rl_grid['n_dates'] == expected_points).all())\n",
    "constraint_pass = True  # run loop 内で assert_long_only を実行済み\n",
    "comparison_pass = bool((method_presence > 0).all().all())\n",
    "\n",
    "perm_numeric_pass = bool(np.isfinite(perm_out['feature_df'][['mean_drop', 'std_drop', 'importance']].to_numpy()).all())\n",
    "ig_numeric_pass = bool(np.isfinite(ig_out['feature_df'][['importance', 'normalized_importance']].to_numpy()).all())\n",
    "perm_sum = float(perm_out['factor_df']['normalized_importance'].sum())\n",
    "ig_sum = float(ig_out['factor_df']['normalized_importance'].sum())\n",
    "shap_sum = float(shap_out['factor_df']['normalized_importance'].sum())\n",
    "perm_sum_pass = bool(abs(perm_sum - 1.0) <= 1e-6)\n",
    "ig_sum_pass = bool(abs(ig_sum - 1.0) <= 1e-6)\n",
    "shap_sum_pass = bool(abs(shap_sum - 1.0) <= 1e-6)\n",
    "\n",
    "perm_repeat_pass = bool(\n",
    "    perm_out['perm_repeats'] == CONFIG['perm_repeats']\n",
    "    and {'mean_drop', 'std_drop'}.issubset(set(perm_out['feature_df'].columns))\n",
    ")\n",
    "ig_config_pass = bool(ig_out['ig_steps'] == CONFIG['ig_steps'] and ig_out['baseline_mode'] == CONFIG['ig_baseline_mode'])\n",
    "\n",
    "compare_methods = set(factor_compare_df['method'].unique().tolist())\n",
    "compare_pass = compare_methods == {'SHAP', 'Permutation', 'IntegratedGradients'}\n",
    "\n",
    "shap_zero_fallback = bool('GradientSaliencyFallback' in shap_out['shap_method'])\n",
    "perm_zero_fallback = bool(float(perm_out['feature_df']['importance'].sum()) <= 0.0)\n",
    "ig_zero_fallback = bool(float(ig_out['feature_df']['importance'].sum()) <= 0.0)\n",
    "\n",
    "\n",
    "def df_to_markdown(df: pd.DataFrame, float_digits: int = 6) -> str:\n",
    "    cols = list(df.columns)\n",
    "    header = '| ' + ' | '.join(cols) + ' |\\n'\n",
    "    sep = '| ' + ' | '.join(['---'] * len(cols)) + ' |\\n'\n",
    "    rows = ''\n",
    "    for _, row in df.iterrows():\n",
    "        vals = []\n",
    "        for c in cols:\n",
    "            v = row[c]\n",
    "            if isinstance(v, (float, np.floating)):\n",
    "                vals.append(f'{float(v):.{float_digits}f}')\n",
    "            else:\n",
    "                vals.append(str(v))\n",
    "        rows += '| ' + ' | '.join(vals) + ' |\\n'\n",
    "    return header + sep + rows\n",
    "\n",
    "\n",
    "metrics_md = df_to_markdown(metrics_df, float_digits=6)\n",
    "shap_top_md = df_to_markdown(shap_out['feature_df'].head(10), float_digits=6)\n",
    "perm_top_md = df_to_markdown(perm_out['feature_df'].head(10), float_digits=6)\n",
    "ig_top_md = df_to_markdown(ig_out['feature_df'].head(10), float_digits=6)\n",
    "factor_compare_md = df_to_markdown(factor_compare_df.sort_values(['method', 'importance'], ascending=[True, False]), float_digits=6)\n",
    "\n",
    "\n",
    "evidence_text = f'''# Phase B v3 Experiment Evidence (No-Fold Single Split)\n",
    "\n",
    "- 実行日時: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "- ノートブック: `/Users/kencharoff/workspace/projects/rl/p03_deep_reinforcement_learning_in_factor_investment/notebooks/cafpo_phase_b_v3_no_fold.ipynb`\n",
    "- split: Train(2001-01〜2020-12) / Test(2021-01〜2025-12)\n",
    "- split_name: `{CONFIG['split_name']}`\n",
    "\n",
    "## 1. 実験条件\n",
    "- 条件行列: PPO/DDPG × [log_return, diff_sharpe, diff_ddr] = 6条件\n",
    "- 制約: Long-only（`w>=0`, `sum(w)=1`）\n",
    "- 報酬適応率: η = {CONFIG['reward_eta']}\n",
    "- DDPG設定: lr={CONFIG['ddpg_learning_rate']}, buffer={CONFIG['ddpg_buffer_size']}, learning_starts={CONFIG['ddpg_learning_starts']}, batch={CONFIG['ddpg_batch_size']}, tau={CONFIG['ddpg_tau']}, gamma={CONFIG['ddpg_gamma']}, train_freq={CONFIG['ddpg_train_freq']}, gradient_steps={CONFIG['ddpg_gradient_steps']}, timesteps={CONFIG['ddpg_total_timesteps']}\n",
    "- Value-weight: 未実装（時価総額データ不在）\n",
    "\n",
    "## 2. 総合比較結果\n",
    "{metrics_md}\n",
    "\n",
    "## 3. 最良条件\n",
    "- best_method: **{best_method}**\n",
    "- best_algo: **{best_algo}**\n",
    "- best_reward: **{best_reward}**\n",
    "- 選定規則: Sharpe最大（同点時 Compound最大）\n",
    "\n",
    "## 4. 解釈性比較（SHAP / Permutation / Integrated Gradients）\n",
    "- SHAP: `abs(SHAP)` を9アクション方向に和し、特徴重要度を算出\n",
    "- Permutation: 各特徴をtest期間でシャッフルし、平均月次リターン低下 `drop = baseline_mean_return - permuted_mean_return` を集計\n",
    "- Integrated Gradients: baseline=`train状態平均`、steps={CONFIG['ig_steps']}、対象スカラー=`softmax(policy(s_t))·r_(t+1)`\n",
    "- 因子集約: 同一因子kの12ラグ合算後、因子間正規化\n",
    "\n",
    "### 4.1 SHAP Top Features\n",
    "{shap_top_md}\n",
    "\n",
    "### 4.2 Permutation Top Features\n",
    "{perm_top_md}\n",
    "\n",
    "### 4.3 Integrated Gradients Top Features\n",
    "{ig_top_md}\n",
    "\n",
    "### 4.4 Factor Contribution Comparison\n",
    "{factor_compare_md}\n",
    "\n",
    "### 4.5 Fallback/設定情報\n",
    "- SHAP手法: {shap_out['shap_method']}\n",
    "- SHAP background/explain: {shap_out['n_background']}/{shap_out['n_explain']}\n",
    "- Permutation repeats: {perm_out['perm_repeats']}, baseline_mean_return={perm_out['baseline_mean_return']:.6f}\n",
    "- IG baseline_mode/steps: {ig_out['baseline_mode']}/{ig_out['ig_steps']}\n",
    "- ゼロ寄与フォールバック: SHAP={'YES' if shap_zero_fallback else 'NO'}, Permutation={'YES' if perm_zero_fallback else 'NO'}, IG={'YES' if ig_zero_fallback else 'NO'}\n",
    "\n",
    "## 5. テスト結果\n",
    "- データ整合テスト: {'PASS' if data_integrity_pass else 'FAIL'}\n",
    "- 時点整合テスト（`X_t` と `r_(t+1)`）: {'PASS' if alignment_pass else 'FAIL'}\n",
    "- 報酬関数テスト（非有限値なし）: {'PASS' if reward_finite_pass else 'FAIL'}\n",
    "- 条件網羅テスト（6条件 × test60か月）: {'PASS' if coverage_pass else 'FAIL'}\n",
    "- 制約テスト（Long-only）: {'PASS' if constraint_pass else 'FAIL'}\n",
    "- 比較整合テスト（同一test dates）: {'PASS' if comparison_pass else 'FAIL'}\n",
    "- 形状整合テスト（3手法 feature 長一致）: {'PASS' if shape_alignment_pass else 'FAIL'}\n",
    "- 数値安定性テスト（Permutation/IG 非有限値なし）: {'PASS' if (perm_numeric_pass and ig_numeric_pass) else 'FAIL'}\n",
    "- Permutation妥当性テスト（repeats/列）: {'PASS' if perm_repeat_pass else 'FAIL'}\n",
    "- IG妥当性テスト（baseline/steps）: {'PASS' if ig_config_pass else 'FAIL'}\n",
    "- 因子寄与合計=1テスト: {'PASS' if (shap_sum_pass and perm_sum_pass and ig_sum_pass) else 'FAIL'}\n",
    "  - SHAP={shap_sum:.6f}, Permutation={perm_sum:.6f}, IG={ig_sum:.6f}\n",
    "- 比較整合テスト（比較CSV method種別）: {'PASS' if compare_pass else 'FAIL'}\n",
    "\n",
    "## 6. 生成物\n",
    "- `/Users/kencharoff/workspace/projects/rl/p03_deep_reinforcement_learning_in_factor_investment/outputs/phase_b_v3_metrics.csv`\n",
    "- `/Users/kencharoff/workspace/projects/rl/p03_deep_reinforcement_learning_in_factor_investment/outputs/phase_b_v3_cumulative_returns.csv`\n",
    "- `/Users/kencharoff/workspace/projects/rl/p03_deep_reinforcement_learning_in_factor_investment/outputs/phase_b_v3_cumulative_returns.png`\n",
    "- `/Users/kencharoff/workspace/projects/rl/p03_deep_reinforcement_learning_in_factor_investment/outputs/phase_b_v3_split_grid.csv`\n",
    "- `/Users/kencharoff/workspace/projects/rl/p03_deep_reinforcement_learning_in_factor_investment/outputs/phase_b_v3_shap_feature_importance.csv`\n",
    "- `/Users/kencharoff/workspace/projects/rl/p03_deep_reinforcement_learning_in_factor_investment/outputs/phase_b_v3_shap_factor_contrib.csv`\n",
    "- `/Users/kencharoff/workspace/projects/rl/p03_deep_reinforcement_learning_in_factor_investment/outputs/phase_b_v3_shap_factor_contrib.png`\n",
    "- `/Users/kencharoff/workspace/projects/rl/p03_deep_reinforcement_learning_in_factor_investment/outputs/phase_b_v3_perm_feature_importance.csv`\n",
    "- `/Users/kencharoff/workspace/projects/rl/p03_deep_reinforcement_learning_in_factor_investment/outputs/phase_b_v3_perm_factor_contrib.csv`\n",
    "- `/Users/kencharoff/workspace/projects/rl/p03_deep_reinforcement_learning_in_factor_investment/outputs/phase_b_v3_perm_factor_contrib.png`\n",
    "- `/Users/kencharoff/workspace/projects/rl/p03_deep_reinforcement_learning_in_factor_investment/outputs/phase_b_v3_ig_feature_importance.csv`\n",
    "- `/Users/kencharoff/workspace/projects/rl/p03_deep_reinforcement_learning_in_factor_investment/outputs/phase_b_v3_ig_factor_contrib.csv`\n",
    "- `/Users/kencharoff/workspace/projects/rl/p03_deep_reinforcement_learning_in_factor_investment/outputs/phase_b_v3_ig_factor_contrib.png`\n",
    "- `/Users/kencharoff/workspace/projects/rl/p03_deep_reinforcement_learning_in_factor_investment/outputs/phase_b_v3_interpretability_factor_compare.csv`\n",
    "- `/Users/kencharoff/workspace/projects/rl/p03_deep_reinforcement_learning_in_factor_investment/outputs/phase_b_v3_experiment_evidence.md`\n",
    "'''\n",
    "\n",
    "evidence_path.write_text(evidence_text, encoding='utf-8')\n",
    "\n",
    "required_outputs = [\n",
    "    metrics_path,\n",
    "    cum_csv_path,\n",
    "    cum_png_path,\n",
    "    split_grid_path,\n",
    "    shap_feature_path,\n",
    "    shap_factor_path,\n",
    "    shap_factor_png_path,\n",
    "    perm_feature_path,\n",
    "    perm_factor_path,\n",
    "    perm_factor_png_path,\n",
    "    ig_feature_path,\n",
    "    ig_factor_path,\n",
    "    ig_factor_png_path,\n",
    "    factor_compare_path,\n",
    "    evidence_path,\n",
    "]\n",
    "for p in required_outputs:\n",
    "    assert p.exists(), f'missing output: {p}'\n",
    "\n",
    "print('SHAP method:', shap_out['shap_method'])\n",
    "print('Permutation repeats:', perm_out['perm_repeats'])\n",
    "print('IG baseline/steps:', ig_out['baseline_mode'], ig_out['ig_steps'])\n",
    "print('factor sums:', shap_sum, perm_sum, ig_sum)\n",
    "print('saved evidence:', evidence_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "\n",
    "- foldベース評価を廃止し、単発split（Train: 2001-2020 / Test: 2021-2025）へ切替。\n",
    "- 比較条件は `PPO/DDPG × 3報酬` の6条件を維持。\n",
    "- 出力は `phase_b_v3_*` として保存し、`phase_b_v2_*` とは分離。\n",
    "- 解釈性は `best_method` 1条件に対し、SHAP + Permutation + Integrated Gradients を併用。\n",
    "- 因子寄与は3手法すべてで12ラグ集約・正規化して比較。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}