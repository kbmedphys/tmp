{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Method Full Notebook\n",
    "\n",
    "この notebook は **単体完結** で、以下を Run All だけで実行します。\n",
    "\n",
    "1. yfinance からテーマETFの価格・出来高を取得（キャッシュ再利用あり）\n",
    "2. 最終採択 proxy（`price_only_mms`）を計算\n",
    "3. Top-K テーマ選抜とETFウェイト構築\n",
    "4. `neutral_mode = none / A_weight_adjust / B_eqw_hedge` を同一条件で比較\n",
    "5. バックテスト（gross/net、turnover、cost、rolling beta）\n",
    "6. train/test 指標表と図表を `outputs/` に保存\n",
    "\n",
    "制約:\n",
    "- `src/` の関数・クラスは **使用しない**（importしない）\n",
    "- `target volatility` は実装しない\n",
    "- 先読み回避（signal窓、beta推定窓、exec_lag）を厳守する\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Problem Setup / Notation\n",
    "\n",
    "本 notebook では、テーマETFローテーションを以下の記号で定義します。\n",
    "\n",
    "- テーマ集合: $\\Theta$\n",
    "- ETF集合: $\\mathcal{I}$\n",
    "- 日次時点: $t$\n",
    "- リバランス時点: $t_r$\n",
    "- テーマ $\\theta$ に属するETF集合: $\\mathcal{I}_\\theta \\subset \\mathcal{I}$\n",
    "\n",
    "ETF $i$ の調整後終値を $P_{i,t}$、日次リターンを\n",
    "\n",
    "$$\n",
    "r_{i,t} = \\frac{P_{i,t}}{P_{i,t-1}} - 1\n",
    "$$\n",
    "\n",
    "とします。\n",
    "\n",
    "取引可能集合はローンチ日制約込みで\n",
    "\n",
    "$$\n",
    "\\mathcal{I}(t)=\\{i\\in\\mathcal{I}\\mid t\\ge \\tau_i^{\\text{launch}},\\ P_{i,t}\\text{ が有効}\\}\n",
    "$$\n",
    "\n",
    "と定義します。ここで $\\tau_i^{\\text{launch}}$ は `first_valid_date` です。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Config\n",
    "\n",
    "THEME_TO_TICKERS = {\n",
    "  \"Ageing Society Opportunities\": [\"AGNG\"],\n",
    "  \"Autonomous Tech & Industrial Innovation\": [\"ARKQ\", \"ROBO\"],\n",
    "  \"Digital Health\": [\"EDOC\"],\n",
    "  \"Efficient Energy\": [\"QCLN\"],\n",
    "  \"Fintech Innovation\": [\"FINX\"],\n",
    "  \"Future Education\": [\"LRNZ\"],\n",
    "  \"Future Mobility\": [\"DRIV\"],\n",
    "  \"Genomic Innovation\": [\"ARKG\"],\n",
    "  \"Millennials\": [\"MILN\"],\n",
    "  \"Next Gen Internet Innovation\": [\"ARKW\"],\n",
    "  \"Robotics\": [\"BOTZ\"],\n",
    "  \"Smart Cities\": [\"KOMP\"],\n",
    "  \"Blockchain Economy\": [\"BLOK\"],\n",
    "  \"Clean Energy Infrastructure\": [\"ICLN\"],\n",
    "  \"Cybersecurity\": [\"HACK\"],\n",
    "  \"Food Revolution\": [\"KROP\"],\n",
    "  \"Natural Resources Stewardship\": [\"WOOD\"],\n",
    "  \"Renewables & Energy Efficiency\": [\"TAN\", \"FAN\"],\n",
    "  \"Sharing Economy\": [\"GIGE\"],\n",
    "  \"Space Exploration\": [\"ARKX\"],\n",
    "  \"Sustainable Water Transition\": [\"PHO\"],\n",
    "}\n",
    "\n",
    "ALL_TICKERS = sorted({t for tickers in THEME_TO_TICKERS.values() for t in tickers})\n",
    "\n",
    "CONFIG = {\n",
    "    \"start\": \"2014-01-01\",\n",
    "    \"end\": None,\n",
    "    \"force_download\": False,\n",
    "    \"rebalance\": \"M\",           # \"M\" or \"Q\"\n",
    "    \"split_date\": \"2021-01-01\",\n",
    "    \"fee_bps\": 10.0,\n",
    "    \"slippage_bps\": 1.0,\n",
    "    \"exec_lag\": 1,\n",
    "    \"top_k\": 4,\n",
    "    \"lookback_3m\": 63,\n",
    "    \"beta_target\": 0.0,\n",
    "    \"beta_lookback\": 60,\n",
    "    \"allow_short_hedge\": False,\n",
    "    \"max_leverage\": 1.5,\n",
    "    \"neutral_modes\": [\"none\", \"A_weight_adjust\", \"B_eqw_hedge\"],\n",
    "    \"force_ff_download\": False,\n",
    "}\n",
    "\n",
    "print(\"tickers:\", len(ALL_TICKERS))\n",
    "print(\"neutral_modes:\", CONFIG[\"neutral_modes\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Imports & Utilities\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(\".\").resolve()\n",
    "MPL_DIR = ROOT / \"outputs\" / \".mplconfig\"\n",
    "MPL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "os.environ.setdefault(\"MPLCONFIGDIR\", str(MPL_DIR))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "\n",
    "DATA_RAW_DIR = ROOT / \"data\" / \"raw\"\n",
    "DATA_PROCESSED_DIR = ROOT / \"data\" / \"processed\"\n",
    "OUT_FIG_DIR = ROOT / \"outputs\" / \"figures\"\n",
    "OUT_TABLE_DIR = ROOT / \"outputs\" / \"tables\"\n",
    "OUT_LOG_DIR = ROOT / \"outputs\" / \"logs\"\n",
    "\n",
    "\n",
    "def ensure_dirs() -> None:\n",
    "    DATA_RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    DATA_PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    OUT_FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    OUT_TABLE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    OUT_LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def align_to_index(target, index, method=\"ffill\"):\n",
    "    if len(index) == 0:\n",
    "        return None\n",
    "    target_ts = pd.Timestamp(target)\n",
    "    loc = index.get_indexer([target_ts], method=method)[0]\n",
    "    if loc == -1:\n",
    "        return None\n",
    "    return pd.Timestamp(index[loc])\n",
    "\n",
    "\n",
    "def _load_cached_data(expected_tickers):\n",
    "    close_path = DATA_RAW_DIR / \"close.csv\"\n",
    "    volume_path = DATA_RAW_DIR / \"volume.csv\"\n",
    "    first_valid_path = DATA_RAW_DIR / \"first_valid_date.csv\"\n",
    "    returns_path = DATA_PROCESSED_DIR / \"returns.csv\"\n",
    "\n",
    "    required = [close_path, volume_path, first_valid_path, returns_path]\n",
    "    if not all(p.exists() for p in required):\n",
    "        return None\n",
    "\n",
    "    close = pd.read_csv(close_path, index_col=0, parse_dates=True).sort_index()\n",
    "    volume = pd.read_csv(volume_path, index_col=0, parse_dates=True).sort_index()\n",
    "    returns = pd.read_csv(returns_path, index_col=0, parse_dates=True).sort_index()\n",
    "\n",
    "    first_valid_raw = pd.read_csv(first_valid_path, index_col=0)\n",
    "    first_valid = {}\n",
    "    for ticker in first_valid_raw.index:\n",
    "        val = first_valid_raw.loc[ticker].iloc[0]\n",
    "        if isinstance(val, str) and val:\n",
    "            first_valid[ticker] = pd.Timestamp(val)\n",
    "        elif pd.notna(val):\n",
    "            first_valid[ticker] = pd.Timestamp(val)\n",
    "        else:\n",
    "            first_valid[ticker] = None\n",
    "\n",
    "    cached_tickers = set(close.columns)\n",
    "    if not set(expected_tickers).issubset(cached_tickers):\n",
    "        return None\n",
    "\n",
    "    close = close.reindex(columns=expected_tickers)\n",
    "    volume = volume.reindex(index=close.index, columns=expected_tickers)\n",
    "    returns = returns.reindex(index=close.index, columns=expected_tickers)\n",
    "    first_valid = {t: first_valid.get(t) for t in expected_tickers}\n",
    "    return close, volume, returns, first_valid\n",
    "\n",
    "\n",
    "def _download_data_yf(tickers, start=None, end=None):\n",
    "    data = yf.download(\n",
    "        tickers=tickers,\n",
    "        start=start,\n",
    "        end=end,\n",
    "        interval=\"1d\",\n",
    "        auto_adjust=True,\n",
    "        progress=False,\n",
    "        group_by=\"column\",\n",
    "        threads=True,\n",
    "    )\n",
    "    if data.empty:\n",
    "        raise ValueError(\"yfinance returned no data.\")\n",
    "\n",
    "    if isinstance(data.columns, pd.MultiIndex):\n",
    "        close = data[\"Close\"].copy()\n",
    "        volume = data[\"Volume\"].copy()\n",
    "    else:\n",
    "        ticker = tickers[0]\n",
    "        close = data[[\"Close\"]].copy()\n",
    "        close.columns = [ticker]\n",
    "        volume = data[[\"Volume\"]].copy()\n",
    "        volume.columns = [ticker]\n",
    "\n",
    "    close = close.sort_index().dropna(how=\"all\").ffill()\n",
    "    volume = volume.reindex(close.index)\n",
    "\n",
    "    close = close.reindex(columns=tickers)\n",
    "    volume = volume.reindex(columns=tickers)\n",
    "    return close, volume\n",
    "\n",
    "\n",
    "def _first_valid_dates(close):\n",
    "    out = {}\n",
    "    for col in close.columns:\n",
    "        d = close[col].first_valid_index()\n",
    "        out[col] = pd.Timestamp(d) if d is not None else None\n",
    "    return out\n",
    "\n",
    "\n",
    "def _apply_launch_mask(close, volume, first_valid):\n",
    "    close_m = close.copy()\n",
    "    volume_m = volume.copy()\n",
    "    for ticker, launch_dt in first_valid.items():\n",
    "        if launch_dt is None:\n",
    "            continue\n",
    "        mask = close_m.index < launch_dt\n",
    "        close_m.loc[mask, ticker] = np.nan\n",
    "        volume_m.loc[mask, ticker] = np.nan\n",
    "    return close_m, volume_m\n",
    "\n",
    "\n",
    "def _save_cache(close, volume, returns, first_valid):\n",
    "    ensure_dirs()\n",
    "    close.to_csv(DATA_RAW_DIR / \"close.csv\")\n",
    "    volume.to_csv(DATA_RAW_DIR / \"volume.csv\")\n",
    "\n",
    "    first_valid_ser = pd.Series({\n",
    "        k: (v.isoformat() if v is not None else \"\")\n",
    "        for k, v in first_valid.items()\n",
    "    })\n",
    "    first_valid_ser.to_csv(DATA_RAW_DIR / \"first_valid_date.csv\", header=[\"first_valid_date\"])\n",
    "\n",
    "    returns.to_csv(DATA_PROCESSED_DIR / \"returns.csv\")\n",
    "\n",
    "\n",
    "def load_or_download_data(tickers, start=None, end=None, force_download=False):\n",
    "    tickers = sorted(set(tickers))\n",
    "    ensure_dirs()\n",
    "\n",
    "    if not force_download:\n",
    "        cached = _load_cached_data(tickers)\n",
    "        if cached is not None:\n",
    "            return cached\n",
    "\n",
    "    close, volume = _download_data_yf(tickers=tickers, start=start, end=end)\n",
    "    first_valid = _first_valid_dates(close)\n",
    "    close, volume = _apply_launch_mask(close, volume, first_valid)\n",
    "    returns = close.pct_change(fill_method=None)\n",
    "\n",
    "    _save_cache(close, volume, returns, first_valid)\n",
    "    return close, volume, returns, first_valid\n",
    "\n",
    "\n",
    "def zscore_cross_section(series: pd.Series) -> pd.Series:\n",
    "    mu = series.mean(skipna=True)\n",
    "    sigma = series.std(skipna=True, ddof=0)\n",
    "    if not np.isfinite(sigma) or sigma < 1e-12:\n",
    "        return series * 0.0\n",
    "    return (series - mu) / sigma\n",
    "\n",
    "\n",
    "def winsorize(series: pd.Series, k: float = 3.0) -> pd.Series:\n",
    "    return series.clip(lower=-k, upper=k)\n",
    "\n",
    "\n",
    "def build_theme_series(close: pd.DataFrame, volume: pd.DataFrame, theme_to_tickers: dict):\n",
    "    etf_returns = close.pct_change(fill_method=None)\n",
    "\n",
    "    theme_returns = {}\n",
    "    theme_dollar_volume = {}\n",
    "\n",
    "    for theme, tickers in theme_to_tickers.items():\n",
    "        cols = [t for t in tickers if t in close.columns]\n",
    "        if not cols:\n",
    "            continue\n",
    "\n",
    "        theme_ret = etf_returns[cols].mean(axis=1, skipna=True)\n",
    "        theme_dv = (close[cols] * volume[cols]).sum(axis=1, min_count=1)\n",
    "\n",
    "        theme_returns[theme] = theme_ret\n",
    "        theme_dollar_volume[theme] = theme_dv\n",
    "\n",
    "    theme_returns_df = pd.DataFrame(theme_returns).sort_index()\n",
    "    theme_dv_df = pd.DataFrame(theme_dollar_volume).reindex(theme_returns_df.index).sort_index()\n",
    "    return theme_returns_df, theme_dv_df\n",
    "\n",
    "\n",
    "def compute_price_only_scores(theme_returns: pd.DataFrame, lookback_3m: int = 63, eps: float = 1e-12):\n",
    "    idx = theme_returns.index\n",
    "    month_ends = theme_returns.resample(\"ME\").last().index\n",
    "    month_ends = pd.DatetimeIndex([d for d in month_ends if d in idx])\n",
    "\n",
    "    score_rows = []\n",
    "    mom_rows = []\n",
    "    prev_rank = None\n",
    "\n",
    "    for date in month_ends:\n",
    "        loc = idx.get_loc(date)\n",
    "        if loc < (2 * lookback_3m + 5):\n",
    "            continue\n",
    "\n",
    "        w0 = slice(loc - lookback_3m + 1, loc + 1)\n",
    "        w1 = slice(loc - 2 * lookback_3m + 1, loc - lookback_3m + 1)\n",
    "\n",
    "        ret_w0 = theme_returns.iloc[w0]\n",
    "        ret_w1 = theme_returns.iloc[w1]\n",
    "\n",
    "        valid_now = theme_returns.loc[date].notna()\n",
    "        valid_history = (\n",
    "            (ret_w0.notna().sum(axis=0) >= lookback_3m)\n",
    "            & (ret_w1.notna().sum(axis=0) >= lookback_3m)\n",
    "        )\n",
    "        valid_theme = valid_now & valid_history\n",
    "\n",
    "        r0 = (1.0 + ret_w0).prod(axis=0, skipna=True) - 1.0\n",
    "        s0 = ret_w0.std(axis=0, ddof=0)\n",
    "        mom0 = r0 / (s0 + eps)\n",
    "\n",
    "        r1 = (1.0 + ret_w1).prod(axis=0, skipna=True) - 1.0\n",
    "        s1 = ret_w1.std(axis=0, ddof=0)\n",
    "        mom1 = r1 / (s1 + eps)\n",
    "\n",
    "        mom_abs = mom0 - mom1\n",
    "        mom_pct = (mom0 - mom1) / (mom1.abs() + eps)\n",
    "\n",
    "        raw = zscore_cross_section(mom_abs) + zscore_cross_section(mom_pct)\n",
    "        score = zscore_cross_section(raw)\n",
    "        score = winsorize(score, k=3.0).round(1)\n",
    "\n",
    "        score = score.where(valid_theme)\n",
    "        mom0 = mom0.where(valid_theme)\n",
    "\n",
    "        if score.notna().any():\n",
    "            if prev_rank is None:\n",
    "                ordered = score.dropna().sort_values(ascending=False, kind=\"mergesort\").index\n",
    "            else:\n",
    "                rank_df = pd.DataFrame({\n",
    "                    \"score\": score,\n",
    "                    \"prev\": prev_rank.reindex(score.index),\n",
    "                })\n",
    "                max_prev = float(prev_rank.max()) if len(prev_rank) > 0 else 0.0\n",
    "                rank_df[\"prev\"] = rank_df[\"prev\"].fillna(max_prev + 1.0)\n",
    "                ordered = (\n",
    "                    rank_df.dropna(subset=[\"score\"])\n",
    "                    .sort_values([\"score\", \"prev\"], ascending=[False, True], kind=\"mergesort\")\n",
    "                    .index\n",
    "                )\n",
    "            prev_rank = pd.Series(np.arange(1, len(ordered) + 1), index=ordered, dtype=float)\n",
    "\n",
    "        score_rows.append(score.rename(date))\n",
    "        mom_rows.append(mom0.rename(date))\n",
    "\n",
    "    score_m = pd.DataFrame(score_rows).sort_index().reindex(columns=theme_returns.columns)\n",
    "    mom_m = pd.DataFrame(mom_rows).sort_index().reindex(columns=theme_returns.columns)\n",
    "    return score_m, mom_m\n",
    "\n",
    "\n",
    "def _normalize_rebalance_freq(freq: str) -> str:\n",
    "    f = str(freq).upper()\n",
    "    if f == \"M\":\n",
    "        return \"ME\"\n",
    "    if f == \"Q\":\n",
    "        return \"QE\"\n",
    "    return f\n",
    "\n",
    "\n",
    "def _theme_has_active_etf(theme: str, date: pd.Timestamp, close: pd.DataFrame, theme_to_tickers: dict) -> bool:\n",
    "    tickers = [t for t in theme_to_tickers.get(theme, []) if t in close.columns]\n",
    "    if not tickers:\n",
    "        return False\n",
    "    row = close.loc[date, tickers]\n",
    "    return bool(row.notna().any())\n",
    "\n",
    "\n",
    "def build_rebalance_theme_weights(score_m, mom_m, close, theme_to_tickers, rebalance=\"M\", top_k=4):\n",
    "    alias = _normalize_rebalance_freq(rebalance)\n",
    "    score_r = score_m.resample(alias).last()\n",
    "    mom_r = mom_m.resample(alias).last()\n",
    "    rebalance_dates = score_r.index\n",
    "\n",
    "    theme_weights = pd.DataFrame(0.0, index=rebalance_dates, columns=score_m.columns)\n",
    "\n",
    "    for date in rebalance_dates:\n",
    "        decision_date = align_to_index(date, close.index, method=\"ffill\")\n",
    "        if decision_date is None:\n",
    "            continue\n",
    "\n",
    "        score = score_r.loc[date].copy()\n",
    "        mom = mom_r.loc[date].copy()\n",
    "\n",
    "        for theme in score.index:\n",
    "            if not _theme_has_active_etf(theme, decision_date, close, theme_to_tickers):\n",
    "                score.loc[theme] = np.nan\n",
    "                mom.loc[theme] = np.nan\n",
    "\n",
    "        selected = []\n",
    "        valid_score = score.dropna()\n",
    "        if not valid_score.empty:\n",
    "            selected = list(valid_score.sort_values(ascending=False, kind=\"mergesort\").head(top_k).index)\n",
    "\n",
    "        if len(selected) < top_k:\n",
    "            valid_mom = mom.dropna().sort_values(ascending=False, kind=\"mergesort\")\n",
    "            for theme in valid_mom.index:\n",
    "                if theme not in selected:\n",
    "                    selected.append(theme)\n",
    "                if len(selected) >= top_k:\n",
    "                    break\n",
    "\n",
    "        if selected:\n",
    "            w = 1.0 / len(selected)\n",
    "            theme_weights.loc[date, selected] = w\n",
    "\n",
    "    return theme_weights\n",
    "\n",
    "\n",
    "def map_theme_to_etf_weights(theme_weights, close, theme_to_tickers):\n",
    "    etf_weights = pd.DataFrame(0.0, index=theme_weights.index, columns=close.columns)\n",
    "\n",
    "    for date in theme_weights.index:\n",
    "        decision_date = align_to_index(date, close.index, method=\"ffill\")\n",
    "        if decision_date is None:\n",
    "            continue\n",
    "\n",
    "        row = theme_weights.loc[date]\n",
    "        for theme, theme_weight in row.items():\n",
    "            if theme_weight <= 0:\n",
    "                continue\n",
    "\n",
    "            tickers = [t for t in theme_to_tickers.get(theme, []) if t in close.columns]\n",
    "            if not tickers:\n",
    "                continue\n",
    "\n",
    "            available = close.loc[decision_date, tickers].dropna().index.tolist()\n",
    "            if not available:\n",
    "                continue\n",
    "\n",
    "            per_etf = theme_weight / len(available)\n",
    "            etf_weights.loc[date, available] += per_etf\n",
    "\n",
    "        total = etf_weights.loc[date].sum()\n",
    "        if total > 0:\n",
    "            etf_weights.loc[date] = etf_weights.loc[date] / total\n",
    "\n",
    "    return etf_weights\n",
    "\n",
    "\n",
    "def expand_weights_daily(rebalance_weights, daily_index, exec_lag_days=1):\n",
    "    daily_weights = pd.DataFrame(np.nan, index=daily_index, columns=rebalance_weights.columns)\n",
    "\n",
    "    for date in rebalance_weights.index:\n",
    "        apply_anchor = align_to_index(date, daily_index, method=\"bfill\")\n",
    "        if apply_anchor is None:\n",
    "            continue\n",
    "\n",
    "        loc = daily_index.get_loc(apply_anchor)\n",
    "        apply_loc = min(loc + int(exec_lag_days), len(daily_index) - 1)\n",
    "        apply_date = daily_index[apply_loc]\n",
    "        daily_weights.loc[apply_date] = rebalance_weights.loc[date]\n",
    "\n",
    "    daily_weights = daily_weights.ffill().fillna(0.0)\n",
    "    return daily_weights\n",
    "\n",
    "\n",
    "def compute_eqw_weights(close):\n",
    "    available = close.notna().astype(float)\n",
    "    counts = available.sum(axis=1)\n",
    "    counts = counts.where(counts > 0)\n",
    "    weights = available.div(counts, axis=0)\n",
    "    return weights.fillna(0.0)\n",
    "\n",
    "\n",
    "def backtest_from_daily_weights(daily_weights, returns, fee_bps=10.0, slippage_bps=1.0):\n",
    "    weights = daily_weights.reindex(returns.index).fillna(0.0)\n",
    "    ret = returns.reindex(weights.index).fillna(0.0)\n",
    "\n",
    "    gross_ret = (weights * ret).sum(axis=1)\n",
    "    turnover = weights.diff().abs().sum(axis=1).fillna(0.0)\n",
    "\n",
    "    cost = turnover * (fee_bps + slippage_bps) * 1e-4\n",
    "    net_ret = gross_ret - cost\n",
    "\n",
    "    out = pd.DataFrame(\n",
    "        {\n",
    "            \"gross_ret\": gross_ret,\n",
    "            \"net_ret\": net_ret,\n",
    "            \"turnover\": turnover,\n",
    "            \"cost\": cost,\n",
    "            \"gross_nav\": (1.0 + gross_ret).cumprod(),\n",
    "            \"net_nav\": (1.0 + net_ret).cumprod(),\n",
    "            \"cum_cost\": cost.cumsum(),\n",
    "            \"leverage\": weights.abs().sum(axis=1),\n",
    "        },\n",
    "        index=weights.index,\n",
    "    )\n",
    "    return out\n",
    "\n",
    "\n",
    "def compute_eqw_backtest(close, returns, fee_bps=10.0, slippage_bps=1.0):\n",
    "    eqw_daily = compute_eqw_weights(close).reindex(returns.index).fillna(0.0)\n",
    "    return backtest_from_daily_weights(eqw_daily, returns, fee_bps=fee_bps, slippage_bps=slippage_bps)\n",
    "\n",
    "\n",
    "def estimate_beta_ols(port_ret: pd.Series, factor_ret: pd.Series) -> float:\n",
    "    aligned = pd.concat([port_ret.rename(\"p\"), factor_ret.rename(\"f\")], axis=1).dropna()\n",
    "    if len(aligned) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    var_f = aligned[\"f\"].var(ddof=0)\n",
    "    if not np.isfinite(var_f) or var_f < 1e-12:\n",
    "        return 0.0\n",
    "\n",
    "    cov_pf = ((aligned[\"p\"] - aligned[\"p\"].mean()) * (aligned[\"f\"] - aligned[\"f\"].mean())).mean()\n",
    "    return float(cov_pf / var_f)\n",
    "\n",
    "\n",
    "def _extract_lookback_window(returns, factor_ret, decision_date, lookback):\n",
    "    end_loc = returns.index.get_loc(decision_date) - 1\n",
    "    start_loc = end_loc - int(lookback) + 1\n",
    "\n",
    "    if start_loc < 0 or end_loc < 0:\n",
    "        return pd.DataFrame(columns=returns.columns), pd.Series(dtype=float), start_loc, end_loc\n",
    "\n",
    "    returns_window = returns.iloc[start_loc : end_loc + 1].fillna(0.0)\n",
    "    factor_window = factor_ret.iloc[start_loc : end_loc + 1].fillna(0.0)\n",
    "    return returns_window, factor_window, start_loc, end_loc\n",
    "\n",
    "\n",
    "def _eqw_vector_on_date(close, date):\n",
    "    vector = pd.Series(0.0, index=close.columns)\n",
    "    row = close.loc[date]\n",
    "    available = row.dropna().index.tolist()\n",
    "    if not available:\n",
    "        return vector\n",
    "    vector.loc[available] = 1.0 / len(available)\n",
    "    return vector\n",
    "\n",
    "\n",
    "def _project_weights(weights, allow_short_hedge, max_leverage, enforce_sum_one):\n",
    "    projected = weights.copy()\n",
    "\n",
    "    if not allow_short_hedge:\n",
    "        projected = projected.clip(lower=0.0)\n",
    "\n",
    "    if enforce_sum_one:\n",
    "        total = projected.sum()\n",
    "        if abs(total) > 1e-12:\n",
    "            projected = projected / total\n",
    "        else:\n",
    "            projected[:] = 0.0\n",
    "\n",
    "    gross = projected.abs().sum()\n",
    "    if gross > max_leverage and gross > 1e-12:\n",
    "        projected = projected * (max_leverage / gross)\n",
    "\n",
    "    return projected\n",
    "\n",
    "\n",
    "def _portfolio_beta(weights, returns_window, factor_window):\n",
    "    port_ret = returns_window.mul(weights, axis=1).sum(axis=1)\n",
    "    return estimate_beta_ols(port_ret, factor_window)\n",
    "\n",
    "\n",
    "def _line_search_lambda(\n",
    "    base_weight,\n",
    "    eqw_vector,\n",
    "    returns_window,\n",
    "    factor_window,\n",
    "    beta_target,\n",
    "    lambda_init,\n",
    "    allow_short_hedge,\n",
    "    max_leverage,\n",
    "    enforce_sum_one,\n",
    "):\n",
    "    if abs(lambda_init) < 1e-10:\n",
    "        coarse = [0.0, 0.05, -0.05]\n",
    "    else:\n",
    "        coarse = [0.0, 0.5 * lambda_init, lambda_init, 1.5 * lambda_init, -0.5 * lambda_init]\n",
    "\n",
    "    best_weight = None\n",
    "    best_lambda = 0.0\n",
    "    best_beta = 0.0\n",
    "    best_error = np.inf\n",
    "\n",
    "    def _evaluate(lmbd):\n",
    "        candidate = base_weight - lmbd * eqw_vector\n",
    "        candidate = _project_weights(\n",
    "            candidate,\n",
    "            allow_short_hedge=allow_short_hedge,\n",
    "            max_leverage=max_leverage,\n",
    "            enforce_sum_one=enforce_sum_one,\n",
    "        )\n",
    "        beta = _portfolio_beta(candidate, returns_window, factor_window)\n",
    "        err = abs(beta - beta_target)\n",
    "        return candidate, beta, err\n",
    "\n",
    "    for lmbd in coarse:\n",
    "        w, beta, err = _evaluate(lmbd)\n",
    "        if err < best_error:\n",
    "            best_weight = w\n",
    "            best_lambda = float(lmbd)\n",
    "            best_beta = float(beta)\n",
    "            best_error = float(err)\n",
    "\n",
    "    step = max(abs(best_lambda) * 0.25, 0.05)\n",
    "    fine = [best_lambda - step, best_lambda, best_lambda + step]\n",
    "    for lmbd in fine:\n",
    "        w, beta, err = _evaluate(lmbd)\n",
    "        if err < best_error:\n",
    "            best_weight = w\n",
    "            best_lambda = float(lmbd)\n",
    "            best_beta = float(beta)\n",
    "            best_error = float(err)\n",
    "\n",
    "    if best_weight is None:\n",
    "        best_weight = _project_weights(\n",
    "            base_weight,\n",
    "            allow_short_hedge=allow_short_hedge,\n",
    "            max_leverage=max_leverage,\n",
    "            enforce_sum_one=enforce_sum_one,\n",
    "        )\n",
    "        best_beta = float(_portfolio_beta(best_weight, returns_window, factor_window))\n",
    "        best_lambda = 0.0\n",
    "\n",
    "    return best_weight, best_beta, best_lambda\n",
    "\n",
    "\n",
    "def apply_beta_neutralization(\n",
    "    rebalance_weights,\n",
    "    daily_weights_unhedged,\n",
    "    close,\n",
    "    returns,\n",
    "    eqw_factor_ret,\n",
    "    neutral_mode=\"none\",\n",
    "    beta_target=0.0,\n",
    "    beta_lookback=60,\n",
    "    allow_short_hedge=False,\n",
    "    max_leverage=1.5,\n",
    "):\n",
    "    if neutral_mode not in {\"none\", \"A_weight_adjust\", \"B_eqw_hedge\"}:\n",
    "        raise ValueError(f\"Unsupported neutral_mode: {neutral_mode}\")\n",
    "\n",
    "    neutral_weights = rebalance_weights.copy()\n",
    "    beta_before = pd.Series(index=rebalance_weights.index, dtype=float)\n",
    "    beta_after = pd.Series(index=rebalance_weights.index, dtype=float)\n",
    "    lambda_used = pd.Series(index=rebalance_weights.index, dtype=float)\n",
    "\n",
    "    if neutral_mode == \"none\":\n",
    "        beta_before[:] = np.nan\n",
    "        beta_after[:] = np.nan\n",
    "        lambda_used[:] = 0.0\n",
    "        return neutral_weights, beta_before, beta_after, lambda_used\n",
    "\n",
    "    unhedged_ret = (\n",
    "        daily_weights_unhedged\n",
    "        * returns.reindex(daily_weights_unhedged.index).fillna(0.0)\n",
    "    ).sum(axis=1)\n",
    "\n",
    "    for date in rebalance_weights.index:\n",
    "        decision_date = align_to_index(date, returns.index, method=\"ffill\")\n",
    "        if decision_date is None:\n",
    "            neutral_weights.loc[date] = rebalance_weights.loc[date]\n",
    "            beta_before.loc[date] = 0.0\n",
    "            beta_after.loc[date] = 0.0\n",
    "            lambda_used.loc[date] = 0.0\n",
    "            continue\n",
    "\n",
    "        returns_window, factor_window, start_loc, end_loc = _extract_lookback_window(\n",
    "            returns=returns,\n",
    "            factor_ret=eqw_factor_ret,\n",
    "            decision_date=decision_date,\n",
    "            lookback=beta_lookback,\n",
    "        )\n",
    "\n",
    "        if start_loc < 0 or end_loc < 0:\n",
    "            beta_hist = 0.0\n",
    "        else:\n",
    "            y = unhedged_ret.iloc[start_loc : end_loc + 1]\n",
    "            x = eqw_factor_ret.iloc[start_loc : end_loc + 1]\n",
    "            beta_hist = estimate_beta_ols(y, x)\n",
    "\n",
    "        base_weight = rebalance_weights.loc[date].copy()\n",
    "        eqw_vector = _eqw_vector_on_date(close, decision_date)\n",
    "\n",
    "        enforce_sum_one = neutral_mode == \"A_weight_adjust\"\n",
    "\n",
    "        if returns_window.empty or factor_window.empty or eqw_vector.abs().sum() < 1e-12:\n",
    "            adjusted = _project_weights(\n",
    "                base_weight,\n",
    "                allow_short_hedge=allow_short_hedge,\n",
    "                max_leverage=max_leverage,\n",
    "                enforce_sum_one=enforce_sum_one,\n",
    "            )\n",
    "            beta_new = 0.0\n",
    "            lambda_star = 0.0\n",
    "        else:\n",
    "            beta_u = _portfolio_beta(eqw_vector, returns_window, factor_window)\n",
    "            if abs(beta_u) < 1e-12:\n",
    "                lambda_init = 0.0\n",
    "            else:\n",
    "                lambda_init = (beta_hist - beta_target) / beta_u\n",
    "\n",
    "            adjusted, beta_new, lambda_star = _line_search_lambda(\n",
    "                base_weight=base_weight,\n",
    "                eqw_vector=eqw_vector,\n",
    "                returns_window=returns_window,\n",
    "                factor_window=factor_window,\n",
    "                beta_target=beta_target,\n",
    "                lambda_init=lambda_init,\n",
    "                allow_short_hedge=allow_short_hedge,\n",
    "                max_leverage=max_leverage,\n",
    "                enforce_sum_one=enforce_sum_one,\n",
    "            )\n",
    "\n",
    "        neutral_weights.loc[date] = adjusted\n",
    "        beta_before.loc[date] = beta_hist\n",
    "        beta_after.loc[date] = beta_new\n",
    "        lambda_used.loc[date] = lambda_star\n",
    "\n",
    "    return neutral_weights, beta_before, beta_after, lambda_used\n",
    "\n",
    "\n",
    "def annualized_return(returns: pd.Series, periods_per_year: int = 252) -> float:\n",
    "    series = returns.dropna()\n",
    "    if series.empty:\n",
    "        return np.nan\n",
    "    gross = (1.0 + series).prod()\n",
    "    years = len(series) / float(periods_per_year)\n",
    "    if years <= 0:\n",
    "        return np.nan\n",
    "    return float(gross ** (1.0 / years) - 1.0)\n",
    "\n",
    "\n",
    "def annualized_volatility(returns: pd.Series, periods_per_year: int = 252) -> float:\n",
    "    series = returns.dropna()\n",
    "    if series.empty:\n",
    "        return np.nan\n",
    "    return float(series.std(ddof=0) * np.sqrt(periods_per_year))\n",
    "\n",
    "\n",
    "def sharpe_ratio(returns: pd.Series, periods_per_year: int = 252) -> float:\n",
    "    ann_ret = annualized_return(returns, periods_per_year=periods_per_year)\n",
    "    ann_vol = annualized_volatility(returns, periods_per_year=periods_per_year)\n",
    "    if not np.isfinite(ann_ret) or not np.isfinite(ann_vol) or ann_vol < 1e-12:\n",
    "        return np.nan\n",
    "    return float(ann_ret / ann_vol)\n",
    "\n",
    "\n",
    "def max_drawdown(nav: pd.Series) -> float:\n",
    "    series = nav.dropna()\n",
    "    if series.empty:\n",
    "        return np.nan\n",
    "    peak = series.cummax()\n",
    "    dd = series / peak - 1.0\n",
    "    return float(dd.min())\n",
    "\n",
    "\n",
    "def rolling_beta(port_ret: pd.Series, factor_ret: pd.Series, lookback: int = 60) -> pd.Series:\n",
    "    aligned = pd.concat([port_ret.rename(\"p\"), factor_ret.rename(\"f\")], axis=1)\n",
    "    cov = aligned[\"p\"].rolling(lookback).cov(aligned[\"f\"])\n",
    "    var = aligned[\"f\"].rolling(lookback).var()\n",
    "    return (cov / var).rename(\"rolling_beta\")\n",
    "\n",
    "\n",
    "def summarize_train_test(results_by_mode, eqw_factor_ret, split_date=\"2021-01-01\", beta_lookback=60):\n",
    "    split_ts = pd.Timestamp(split_date)\n",
    "    rows = []\n",
    "\n",
    "    for mode, payload in results_by_mode.items():\n",
    "        bt = payload[\"bt\"]\n",
    "        beta_series = rolling_beta(bt[\"net_ret\"], eqw_factor_ret, lookback=beta_lookback)\n",
    "\n",
    "        period_map = {\n",
    "            \"all\": bt.index,\n",
    "            \"train\": bt.index[bt.index < split_ts],\n",
    "            \"test\": bt.index[bt.index >= split_ts],\n",
    "        }\n",
    "\n",
    "        for period_name, idx in period_map.items():\n",
    "            frame = bt.loc[idx]\n",
    "            if frame.empty:\n",
    "                rows.append(\n",
    "                    {\n",
    "                        \"neutral_mode\": mode,\n",
    "                        \"period\": period_name,\n",
    "                        \"cagr\": np.nan,\n",
    "                        \"ann_vol\": np.nan,\n",
    "                        \"sharpe\": np.nan,\n",
    "                        \"mdd\": np.nan,\n",
    "                        \"avg_turnover\": np.nan,\n",
    "                        \"total_cost\": np.nan,\n",
    "                        \"avg_beta\": np.nan,\n",
    "                    }\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"neutral_mode\": mode,\n",
    "                    \"period\": period_name,\n",
    "                    \"cagr\": annualized_return(frame[\"net_ret\"]),\n",
    "                    \"ann_vol\": annualized_volatility(frame[\"net_ret\"]),\n",
    "                    \"sharpe\": sharpe_ratio(frame[\"net_ret\"]),\n",
    "                    \"mdd\": max_drawdown(frame[\"net_nav\"]),\n",
    "                    \"avg_turnover\": float(frame[\"turnover\"].mean()),\n",
    "                    \"total_cost\": float(frame[\"cost\"].sum()),\n",
    "                    \"avg_beta\": float(beta_series.reindex(frame.index).mean(skipna=True)),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def summarize_mode_all(results_by_mode, eqw_factor_ret, beta_lookback=60):\n",
    "    rows = []\n",
    "    for mode, payload in results_by_mode.items():\n",
    "        bt = payload[\"bt\"]\n",
    "        rb = rolling_beta(bt[\"net_ret\"], eqw_factor_ret, lookback=beta_lookback)\n",
    "        rows.append(\n",
    "            {\n",
    "                \"neutral_mode\": mode,\n",
    "                \"ann_return_net\": annualized_return(bt[\"net_ret\"]),\n",
    "                \"ann_return_gross\": annualized_return(bt[\"gross_ret\"]),\n",
    "                \"sharpe_net\": sharpe_ratio(bt[\"net_ret\"]),\n",
    "                \"mdd\": max_drawdown(bt[\"net_nav\"]),\n",
    "                \"avg_turnover\": float(bt[\"turnover\"].mean()),\n",
    "                \"total_cost\": float(bt[\"cost\"].sum()),\n",
    "                \"avg_beta\": float(rb.mean(skipna=True)),\n",
    "                \"avg_leverage\": float(bt[\"leverage\"].mean()),\n",
    "                \"final_nav_net\": float(bt[\"net_nav\"].iloc[-1]),\n",
    "            }\n",
    "        )\n",
    "    return pd.DataFrame(rows).sort_values(\"neutral_mode\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "ensure_dirs()\n",
    "print(\"utility functions ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) ETF Launch Date Handling（投資可能性）\n",
    "\n",
    "各ETFについて `first_valid_date` を\n",
    "\n",
    "$$\n",
    "\\tau_i^{\\text{launch}} = \\min\\{t\\mid P_{i,t}\\text{ が欠損でない}\\}\n",
    "$$\n",
    "\n",
    "として定義し、$t<\\tau_i^{\\text{launch}}$ は **投資不可** として価格・出来高を欠損（NaN）のまま保持します。\n",
    "\n",
    "- ローンチ前を 0 リターンで埋めると、非存在期間を「無変動資産」と誤認してバイアスが入るため採用しません。\n",
    "- 実装ではローンチ前を欠損で保持し、ウェイト計算時に「その時点で有効なETFのみ」を対象化します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Download Data\n",
    "\n",
    "close, volume, returns, first_valid_date = load_or_download_data(\n",
    "    tickers=ALL_TICKERS,\n",
    "    start=CONFIG[\"start\"],\n",
    "    end=CONFIG[\"end\"],\n",
    "    force_download=CONFIG[\"force_download\"],\n",
    ")\n",
    "\n",
    "print(\"close shape:\", close.shape)\n",
    "print(\"volume shape:\", volume.shape)\n",
    "print(\"returns shape:\", returns.shape)\n",
    "print(\"sample first_valid_date:\")\n",
    "print(pd.Series(first_valid_date).sort_values().head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Signal / Proxy Definition（最終採択proxy）\n",
    "\n",
    "最終採択proxyは `price_only_mms` です（`docs/notes/70_final_spec.md` 準拠）。\n",
    "\n",
    "まずテーマ日次リターンを、投資可能ETFの等ウェイト平均で作ります。\n",
    "\n",
    "$$\n",
    "r^{\\text{theme}}_{\\theta,t} = \\frac{1}{|\\mathcal{I}_\\theta(t)|}\\sum_{i\\in\\mathcal{I}_\\theta(t)} r_{i,t}\n",
    "$$\n",
    "\n",
    "月末 $m$ で直近窓 $W_0$（63営業日）と一つ前の窓 $W_1$（63営業日）を使い、\n",
    "\n",
    "$$\n",
    "R_{\\theta,j}=\\prod_{t\\in W_j}(1+r^{\\text{theme}}_{\\theta,t})-1,\\quad\n",
    "\\sigma_{\\theta,j}=\\mathrm{std}_{t\\in W_j}(r^{\\text{theme}}_{\\theta,t}),\\quad\n",
    "mom_{\\theta,j}=\\frac{R_{\\theta,j}}{\\sigma_{\\theta,j}+\\varepsilon}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Delta^{abs}_{\\theta}=mom_{\\theta,0}-mom_{\\theta,1},\\quad\n",
    "\\Delta^{pct}_{\\theta}=\\frac{mom_{\\theta,0}-mom_{\\theta,1}}{|mom_{\\theta,1}|+\\varepsilon}\n",
    "$$\n",
    "\n",
    "とし、横断標準化後に\n",
    "\n",
    "$$\n",
    "score_{\\theta,m}=\\mathrm{round}_{0.1}\\left(\\mathrm{clip}\\left(z\\left(z(\\Delta^{abs}_{\\theta})+z(\\Delta^{pct}_{\\theta})\\right),-3,3\\right)\\right)\n",
    "$$\n",
    "\n",
    "をテーマスコアとして使います。\n",
    "\n",
    "一般形としては\n",
    "\n",
    "$$\n",
    "\\mathrm{ThemeScore}_{\\theta,t}=\\frac{1}{|\\mathcal{I}_\\theta(t)|}\\sum_{i\\in\\mathcal{I}_\\theta(t)} \\mathrm{Score}_{i,t}\n",
    "$$\n",
    "\n",
    "ですが、本 notebook 実装では ETF個別スコアを明示計算せず、$r^{\\text{theme}}_{\\theta,t}$ を先に構成して $score_{\\theta,m}$ を直接求めます。\n",
    "\n",
    "**直感**: 「最近3か月のリスク調整モメンタムが、その前3か月に対してどれだけ加速したか」を計る proxy です。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Build Signals (final adopted proxy: price_only_mms)\n",
    "\n",
    "theme_returns, theme_dollar_volume = build_theme_series(\n",
    "    close=close,\n",
    "    volume=volume,\n",
    "    theme_to_tickers=THEME_TO_TICKERS,\n",
    ")\n",
    "\n",
    "score_m, mom_m = compute_price_only_scores(\n",
    "    theme_returns=theme_returns,\n",
    "    lookback_3m=CONFIG[\"lookback_3m\"],\n",
    ")\n",
    "\n",
    "print(\"theme_returns shape:\", theme_returns.shape)\n",
    "print(\"score_m shape:\", score_m.shape)\n",
    "print(\"latest score snapshot:\")\n",
    "print(score_m.tail(1).T.sort_values(score_m.tail(1).index[0], ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Selection Rule（Top-K とリバランス）\n",
    "\n",
    "リバランス時点 $t_r$ は `rebalance` で決まり、デフォルトは月次（`M`）です。\n",
    "\n",
    "有効テーマ集合を $\\mathcal{K}(t_r)$ とすると、選抜集合は\n",
    "\n",
    "$$\n",
    "\\mathcal{S}(t_r)=\\mathrm{TopK}_{\\theta\\in\\mathcal{K}(t_r)}\\left(score_{\\theta,t_r}\\right),\\quad K=\\text{top\\_k}\n",
    "$$\n",
    "\n",
    "です。実装では同点や欠損で不足時、当月の $mom_{\\theta,0}$ 順で補完します。\n",
    "\n",
    "先読み回避のため `exec_lag` を適用し、$t_r$ で決めたウェイトは\n",
    "\n",
    "$$\n",
    "\\text{first tradable date at/after } t_r + \\text{lag}\n",
    "$$\n",
    "\n",
    "から反映されます（決定日当日に即時適用しません）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Build Monthly/Period Rebalance Schedule\n",
    "\n",
    "theme_weights = build_rebalance_theme_weights(\n",
    "    score_m=score_m,\n",
    "    mom_m=mom_m,\n",
    "    close=close,\n",
    "    theme_to_tickers=THEME_TO_TICKERS,\n",
    "    rebalance=CONFIG[\"rebalance\"],\n",
    "    top_k=CONFIG[\"top_k\"],\n",
    ")\n",
    "\n",
    "print(\"rebalance points:\", len(theme_weights))\n",
    "print(\"non-zero themes on last rebalance:\")\n",
    "print(theme_weights.iloc[-1][theme_weights.iloc[-1] > 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) Portfolio Weight Construction（等ウェイト）\n",
    "\n",
    "選抜テーマ間は等ウェイト、テーマ内ETFも等ウェイトです。\n",
    "\n",
    "$$\n",
    "w^{\\text{theme}}_{\\theta,t_r}=\\begin{cases}\n",
    "\\frac{1}{|\\mathcal{S}(t_r)|}, & \\theta\\in\\mathcal{S}(t_r)\\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "ETFレベルでは\n",
    "\n",
    "$$\n",
    "w^{\\text{raw}}_{i,t_r}=\\sum_{\\theta\\in\\mathcal{S}(t_r)} w^{\\text{theme}}_{\\theta,t_r}\n",
    "\\cdot \\frac{\\mathbf{1}\\{i\\in\\mathcal{I}_\\theta(t_r)\\}}{|\\mathcal{I}_\\theta(t_r)|}\n",
    "$$\n",
    "\n",
    "を計算し、合計が正の場合は $\\sum_i w^{\\text{raw}}_{i,t_r}=1$ に正規化します。\n",
    "\n",
    "ベンチマークEQWは投資可能ETFのみで\n",
    "\n",
    "$$\n",
    "b_{i,t}=\\begin{cases}\n",
    "\\frac{1}{|\\mathcal{I}(t)|}, & i\\in\\mathcal{I}(t)\\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases},\\quad\n",
    "r^{EQW}_t=\\sum_i b_{i,t}r_{i,t}\n",
    "$$\n",
    "\n",
    "と定義します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Construct Weights (Top-K -> ETF equal weight)\n",
    "\n",
    "etf_weights_raw = map_theme_to_etf_weights(\n",
    "    theme_weights=theme_weights,\n",
    "    close=close,\n",
    "    theme_to_tickers=THEME_TO_TICKERS,\n",
    ")\n",
    "\n",
    "daily_weights_unhedged = expand_weights_daily(\n",
    "    rebalance_weights=etf_weights_raw,\n",
    "    daily_index=returns.index,\n",
    "    exec_lag_days=CONFIG[\"exec_lag\"],\n",
    ")\n",
    "\n",
    "print(\"rebalance ETF weight rows:\", etf_weights_raw.shape)\n",
    "print(\"daily unhedged weights shape:\", daily_weights_unhedged.shape)\n",
    "print(\"last rebalance weight sum:\", float(etf_weights_raw.iloc[-1].sum()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (6) Beta Estimation vs EQW（先読み回避）\n",
    "\n",
    "各リバランス意思決定時点で、EQWリターンを因子として過去窓のみで beta を推定します。\n",
    "\n",
    "- ルックバック長: $L_\\beta =$ `beta_lookback`\n",
    "- 使用データ: $[t_r-L_\\beta,\\ t_r-1]$（実装上は取引日で前方詰めした decision date の前日まで）\n",
    "\n",
    "$$\n",
    "\\beta_{t_r}=\\frac{\\mathrm{Cov}\\left(r^{port}_{t_r-L_\\beta:t_r-1},\\ r^{EQW}_{t_r-L_\\beta:t_r-1}\\right)}{\\mathrm{Var}\\left(r^{EQW}_{t_r-L_\\beta:t_r-1}\\right)}\n",
    "$$\n",
    "\n",
    "未来のリターンは一切使いません。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (7) Neutralization Modes（none / A / B）\n",
    "\n",
    "EQW方向ベクトルを $u(t_r)$ とし、投資可能ETFにのみ等ウェイトを置きます。\n",
    "\n",
    "$$\n",
    "u_i(t_r)=\\begin{cases}\n",
    "\\frac{1}{|\\mathcal{I}(t_r)|}, & i\\in\\mathcal{I}(t_r)\\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "初期ヘッジ係数は\n",
    "\n",
    "$$\n",
    "\\lambda^{init}_{t_r}=\\frac{\\beta^{hist}_{t_r}-\\beta^{target}}{\\beta_u},\\quad\n",
    "\\beta_u=\\beta(u, r^{EQW})\n",
    "$$\n",
    "\n",
    "で作り、実装では coarse/fine の簡易ラインサーチで $|\\beta-\\beta^{target}|$ を最小化する $\\lambda$ を選びます。\n",
    "\n",
    "### mode = `none`\n",
    "\n",
    "$$\n",
    "w'_{t_r}=w^{raw}_{t_r}\n",
    "$$\n",
    "\n",
    "（調整なし）。\n",
    "\n",
    "### mode = `A_weight_adjust`\n",
    "\n",
    "$$\n",
    "w'_{t_r}=\\Pi_A\\left(w^{raw}_{t_r}-\\lambda_{t_r}u(t_r)\\right)\n",
    "$$\n",
    "\n",
    "- `allow_short_hedge=False` のとき負ウェイトを 0 にクリップ\n",
    "- 合計を 1 に再正規化（`enforce_sum_one=True`）\n",
    "- 総レバレッジ $\\|w\\|_1$ が `max_leverage` 超過時は比例縮小\n",
    "\n",
    "### mode = `B_eqw_hedge`\n",
    "\n",
    "$$\n",
    "w'_{t_r}=\\Pi_B\\left(w^{raw}_{t_r}-\\lambda_{t_r}u(t_r)\\right)\n",
    "$$\n",
    "\n",
    "- 負ウェイト許容は `allow_short_hedge` で制御\n",
    "- `B` では合計1への強制正規化を行わない（`enforce_sum_one=False`）\n",
    "- ただし $\\|w\\|_1 \\le \\text{max\\_leverage}$ は常に満たすよう射影\n",
    "\n",
    "制約で厳密一致できない場合は、上記射影後に「最も近い feasible」解を採用します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Apply Neutralization (none / A_weight_adjust / B_eqw_hedge)\n",
    "\n",
    "# EQW factor for beta estimation (costless factor return)\n",
    "eqw_factor_bt = compute_eqw_backtest(\n",
    "    close=close,\n",
    "    returns=returns,\n",
    "    fee_bps=0.0,\n",
    "    slippage_bps=0.0,\n",
    ")\n",
    "eqw_factor_ret = eqw_factor_bt[\"net_ret\"]\n",
    "\n",
    "neutralized_payload = {}\n",
    "for mode in CONFIG[\"neutral_modes\"]:\n",
    "    rebalance_w, beta_before, beta_after, lambda_used = apply_beta_neutralization(\n",
    "        rebalance_weights=etf_weights_raw,\n",
    "        daily_weights_unhedged=daily_weights_unhedged,\n",
    "        close=close,\n",
    "        returns=returns,\n",
    "        eqw_factor_ret=eqw_factor_ret,\n",
    "        neutral_mode=mode,\n",
    "        beta_target=CONFIG[\"beta_target\"],\n",
    "        beta_lookback=CONFIG[\"beta_lookback\"],\n",
    "        allow_short_hedge=CONFIG[\"allow_short_hedge\"],\n",
    "        max_leverage=CONFIG[\"max_leverage\"],\n",
    "    )\n",
    "\n",
    "    daily_w = expand_weights_daily(\n",
    "        rebalance_weights=rebalance_w,\n",
    "        daily_index=returns.index,\n",
    "        exec_lag_days=CONFIG[\"exec_lag\"],\n",
    "    )\n",
    "\n",
    "    neutralized_payload[mode] = {\n",
    "        \"rebalance_weights\": rebalance_w,\n",
    "        \"daily_weights\": daily_w,\n",
    "        \"beta_before\": beta_before,\n",
    "        \"beta_after\": beta_after,\n",
    "        \"lambda_used\": lambda_used,\n",
    "    }\n",
    "\n",
    "beta_diag = pd.DataFrame(\n",
    "    {\n",
    "        mode: {\n",
    "            \"mean_beta_before\": float(payload[\"beta_before\"].mean(skipna=True)),\n",
    "            \"mean_beta_after\": float(payload[\"beta_after\"].mean(skipna=True)),\n",
    "            \"mean_lambda\": float(payload[\"lambda_used\"].mean(skipna=True)),\n",
    "        }\n",
    "        for mode, payload in neutralized_payload.items()\n",
    "    }\n",
    ").T\n",
    "\n",
    "print(beta_diag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (8) Turnover & Transaction Costs\n",
    "\n",
    "日次ポートフォリオ収益は\n",
    "\n",
    "$$\n",
    "r^{gross}_t=\\sum_i w_{i,t}r_{i,t}\n",
    "$$\n",
    "\n",
    "turnover は\n",
    "\n",
    "$$\n",
    "TO_t=\\sum_i |w_{i,t}-w_{i,t-1}|\n",
    "$$\n",
    "\n",
    "片道コスト係数を\n",
    "\n",
    "$$\n",
    "c=(\\text{fee\\_bps}+\\text{slippage\\_bps})\\times 10^{-4}\n",
    "$$\n",
    "\n",
    "とすると、実装は日次控除で\n",
    "\n",
    "$$\n",
    "r^{net}_t=r^{gross}_t-c\\,TO_t\n",
    "$$\n",
    "\n",
    "です（`cost_t = c·TO_t`, `cum_cost_t = \\sum_{\\tau\\le t} cost_\\tau`）。\n",
    "\n",
    "EQWも同様の式で評価し、beta推定用因子では `fee=slippage=0` を使います。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) Backtest (gross/net, turnover, cost)\n",
    "\n",
    "results_by_mode = {}\n",
    "for mode in CONFIG[\"neutral_modes\"]:\n",
    "    bt = backtest_from_daily_weights(\n",
    "        daily_weights=neutralized_payload[mode][\"daily_weights\"],\n",
    "        returns=returns,\n",
    "        fee_bps=CONFIG[\"fee_bps\"],\n",
    "        slippage_bps=CONFIG[\"slippage_bps\"],\n",
    "    )\n",
    "    rb = rolling_beta(bt[\"net_ret\"], eqw_factor_ret, lookback=CONFIG[\"beta_lookback\"])\n",
    "\n",
    "    results_by_mode[mode] = {\n",
    "        \"bt\": bt,\n",
    "        \"rolling_beta\": rb,\n",
    "        \"beta_before\": neutralized_payload[mode][\"beta_before\"],\n",
    "        \"beta_after\": neutralized_payload[mode][\"beta_after\"],\n",
    "        \"lambda_used\": neutralized_payload[mode][\"lambda_used\"],\n",
    "    }\n",
    "\n",
    "# Benchmark EQW (same cost setup as strategy for gross/net comparison)\n",
    "eqw_bt = compute_eqw_backtest(\n",
    "    close=close,\n",
    "    returns=returns,\n",
    "    fee_bps=CONFIG[\"fee_bps\"],\n",
    "    slippage_bps=CONFIG[\"slippage_bps\"],\n",
    ")\n",
    "\n",
    "print(\"backtest done for modes:\", list(results_by_mode.keys()))\n",
    "print(\"eqw final net nav:\", float(eqw_bt[\"net_nav\"].iloc[-1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (9) Outputs & Evaluation\n",
    "\n",
    "`split_date` で train/test を分割し、modeごとに以下を計算します。\n",
    "\n",
    "- CAGR: $\\left(\\prod_t (1+r_t)\\right)^{252/T}-1$\n",
    "- 年率ボラ: $\\sigma(r_t)\\sqrt{252}$\n",
    "- Sharpe: $\\mathrm{CAGR}/\\mathrm{AnnVol}$\n",
    "- MDD: $\\min_t\\left(\\frac{NAV_t}{\\max_{\\tau\\le t}NAV_\\tau}-1\\right)$\n",
    "- 平均turnover, 総コスト, rolling beta（対EQW）\n",
    "\n",
    "比較は `none / A_weight_adjust / B_eqw_hedge` を同一条件で行い、主に以下で確認します。\n",
    "\n",
    "- 表: `outputs/tables/final_method_train_test_metrics.csv`\n",
    "- 表: `outputs/tables/final_method_mode_summary.csv`\n",
    "- 図: `outputs/figures/final_method_nav_none_A_B_vs_eqw.png`\n",
    "- 図: `outputs/figures/final_method_rolling_beta_none_A_B.png`\n",
    "- 図: `outputs/figures/final_method_turnover_none_A_B.png`\n",
    "- 図: `outputs/figures/final_method_cumulative_cost_none_A_B.png`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) Evaluate (train/test)\n",
    "\n",
    "metrics_train_test = summarize_train_test(\n",
    "    results_by_mode=results_by_mode,\n",
    "    eqw_factor_ret=eqw_factor_ret,\n",
    "    split_date=CONFIG[\"split_date\"],\n",
    "    beta_lookback=CONFIG[\"beta_lookback\"],\n",
    ")\n",
    "\n",
    "mode_summary_all = summarize_mode_all(\n",
    "    results_by_mode=results_by_mode,\n",
    "    eqw_factor_ret=eqw_factor_ret,\n",
    "    beta_lookback=CONFIG[\"beta_lookback\"],\n",
    ")\n",
    "\n",
    "eqw_summary = pd.DataFrame(\n",
    "    {\n",
    "        \"benchmark\": [\"EQW\"],\n",
    "        \"ann_return_net\": [annualized_return(eqw_bt[\"net_ret\"])],\n",
    "        \"ann_return_gross\": [annualized_return(eqw_bt[\"gross_ret\"])],\n",
    "        \"sharpe_net\": [sharpe_ratio(eqw_bt[\"net_ret\"])],\n",
    "        \"mdd\": [max_drawdown(eqw_bt[\"net_nav\"])],\n",
    "        \"avg_turnover\": [float(eqw_bt[\"turnover\"].mean())],\n",
    "        \"total_cost\": [float(eqw_bt[\"cost\"].sum())],\n",
    "        \"avg_beta\": [1.0],\n",
    "        \"avg_leverage\": [float(eqw_bt[\"leverage\"].mean())],\n",
    "        \"final_nav_net\": [float(eqw_bt[\"net_nav\"].iloc[-1])],\n",
    "    }\n",
    ")\n",
    "\n",
    "metrics_train_test_path = OUT_TABLE_DIR / \"final_method_train_test_metrics.csv\"\n",
    "mode_summary_path = OUT_TABLE_DIR / \"final_method_mode_summary.csv\"\n",
    "eqw_summary_path = OUT_TABLE_DIR / \"final_method_eqw_summary.csv\"\n",
    "\n",
    "metrics_train_test.to_csv(metrics_train_test_path, index=False)\n",
    "mode_summary_all.to_csv(mode_summary_path, index=False)\n",
    "eqw_summary.to_csv(eqw_summary_path, index=False)\n",
    "\n",
    "print(\"saved:\", metrics_train_test_path)\n",
    "print(\"saved:\", mode_summary_path)\n",
    "print(\"saved:\", eqw_summary_path)\n",
    "\n",
    "print(\"\\nmode summary (all period):\")\n",
    "print(mode_summary_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12) Weights Export & Visualization\n",
    "\n",
    "### ウェイト定義\n",
    "本 notebook の日次ETFウェイトを $w_{i,t}$ とする。テーマ $\\theta$ の集合を $\\mathcal{I}_\\theta$ とすると、\n",
    "\n",
    "$$\n",
    "W^{\\text{theme}}_{\\theta,t}=\\sum_{i\\in\\mathcal{I}_\\theta} w_{i,t}\n",
    "$$\n",
    "\n",
    "でテーマウェイトを定義する。\n",
    "\n",
    "### テーマ集計方法\n",
    "- `weights_etf_<mode>.csv`: ETF日次ウェイト時系列\n",
    "- `weights_theme_<mode>.csv`: 上式で集計したテーマ日次ウェイト時系列\n",
    "\n",
    "### ローンチ日による有効集合の変化\n",
    "ETFローンチ日前は欠損で扱うため、時点 $t$ で有効なETF集合 $\\mathcal{I}(t)$ は時間変化する。したがってテーマ有効集合も時変であり、初期期間ほど投資可能テーマは限定される。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12) Weights Export & Visualization\n",
    "\n",
    "\n",
    "def aggregate_theme_weights_from_etf(etf_weights: pd.DataFrame, theme_to_tickers: dict) -> pd.DataFrame:\n",
    "    theme_weights = pd.DataFrame(0.0, index=etf_weights.index, columns=list(theme_to_tickers.keys()))\n",
    "    for theme, tickers in theme_to_tickers.items():\n",
    "        cols = [t for t in tickers if t in etf_weights.columns]\n",
    "        if cols:\n",
    "            theme_weights[theme] = etf_weights[cols].sum(axis=1)\n",
    "    return theme_weights\n",
    "\n",
    "\n",
    "weights_export_log = []\n",
    "theme_weights_by_mode = {}\n",
    "etf_weights_by_mode = {}\n",
    "\n",
    "for mode in CONFIG[\"neutral_modes\"]:\n",
    "    etf_daily = neutralized_payload[mode][\"daily_weights\"].copy()\n",
    "    etf_weights_by_mode[mode] = etf_daily\n",
    "\n",
    "    etf_path = OUT_TABLE_DIR / f\"weights_etf_{mode}.csv\"\n",
    "    etf_daily.to_csv(etf_path)\n",
    "\n",
    "    theme_daily = aggregate_theme_weights_from_etf(etf_daily, THEME_TO_TICKERS)\n",
    "    theme_weights_by_mode[mode] = theme_daily\n",
    "\n",
    "    theme_path = OUT_TABLE_DIR / f\"weights_theme_{mode}.csv\"\n",
    "    theme_daily.to_csv(theme_path)\n",
    "\n",
    "    weights_export_log.append({\"mode\": mode, \"etf_path\": str(etf_path), \"theme_path\": str(theme_path)})\n",
    "\n",
    "    # Heatmap uses monthly average weights for readability.\n",
    "    theme_monthly = theme_daily.resample(\"ME\").mean()\n",
    "    mat = theme_monthly.T.values\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    vmax = float(np.nanmax(mat)) if np.isfinite(np.nanmax(mat)) else 0.25\n",
    "    vmax = max(vmax, 0.25)\n",
    "    im = ax.imshow(mat, aspect=\"auto\", cmap=\"YlGnBu\", vmin=0.0, vmax=vmax)\n",
    "\n",
    "    y_idx = np.arange(len(theme_monthly.columns))\n",
    "    ax.set_yticks(y_idx)\n",
    "    ax.set_yticklabels(theme_monthly.columns, fontsize=8)\n",
    "\n",
    "    if len(theme_monthly.index) > 1:\n",
    "        x_idx = np.linspace(0, len(theme_monthly.index) - 1, num=min(10, len(theme_monthly.index)), dtype=int)\n",
    "        x_lbl = [theme_monthly.index[i].strftime(\"%Y-%m\") for i in x_idx]\n",
    "        ax.set_xticks(x_idx)\n",
    "        ax.set_xticklabels(x_lbl, rotation=45, ha=\"right\")\n",
    "\n",
    "    ax.set_title(f\"Theme Weights Heatmap ({mode})\")\n",
    "    ax.set_xlabel(\"Month\")\n",
    "    ax.set_ylabel(\"Theme\")\n",
    "    fig.colorbar(im, ax=ax, fraction=0.02, pad=0.01, label=\"weight\")\n",
    "\n",
    "    heatmap_path = OUT_FIG_DIR / f\"theme_weights_heatmap_{mode}.png\"\n",
    "    fig.savefig(heatmap_path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "# Top-K timeline at rebalance dates (mode-independent selection rule).\n",
    "selected_long = (\n",
    "    theme_weights.stack()\n",
    "    .rename(\"weight\")\n",
    "    .reset_index()\n",
    "    .rename(columns={\"level_0\": \"date\", \"level_1\": \"theme\"})\n",
    ")\n",
    "selected_long = selected_long[selected_long[\"weight\"] > 0].copy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "if not selected_long.empty:\n",
    "    themes = sorted(selected_long[\"theme\"].unique().tolist())\n",
    "    theme_to_y = {theme: idx for idx, theme in enumerate(themes)}\n",
    "\n",
    "    for theme in themes:\n",
    "        dsub = selected_long[selected_long[\"theme\"] == theme]\n",
    "        ax.scatter(dsub[\"date\"], [theme_to_y[theme]] * len(dsub), s=16, label=theme)\n",
    "\n",
    "    ax.set_yticks(list(theme_to_y.values()))\n",
    "    ax.set_yticklabels(list(theme_to_y.keys()), fontsize=8)\n",
    "\n",
    "ax.set_title(\"Top-K Theme Timeline (Rebalance Dates)\")\n",
    "ax.set_xlabel(\"date\")\n",
    "ax.set_ylabel(\"selected theme\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "timeline_path = OUT_FIG_DIR / \"theme_weights_topk_timeline.png\"\n",
    "fig.savefig(timeline_path, dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "weights_export_df = pd.DataFrame(weights_export_log)\n",
    "weights_export_df.to_csv(OUT_TABLE_DIR / \"weights_export_log.csv\", index=False)\n",
    "\n",
    "print(\"saved weight tables/figures\")\n",
    "print(weights_export_df)\n",
    "print(\"timeline:\", timeline_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13) FF Attribution（Ken French Data Library）\n",
    "\n",
    "この節では Fama-French 因子を自動取得し、戦略月次リターンを事後分析する。\n",
    "\n",
    "### 月次化\n",
    "日次リターン $r_d$ を月次へ変換:\n",
    "\n",
    "$$\n",
    "r_m = \\prod_{d\\in m}(1+r_d)-1\n",
    "$$\n",
    "\n",
    "### excess return\n",
    "戦略 net 月次リターンを $r^{str}_m$、無リスクを $RF_m$ として\n",
    "\n",
    "$$\n",
    "y_m = r^{str}_m - RF_m\n",
    "$$\n",
    "\n",
    "を回帰対象とする。追加で active return proxy として\n",
    "\n",
    "$$\n",
    "r^{active}_m = r^{str}_m - r^{EQW}_m\n",
    "$$\n",
    "\n",
    "も分析する（こちらは自己資金型差分として RF 控除しない）。\n",
    "\n",
    "### 回帰モデル\n",
    "\n",
    "- FF3:\n",
    "$$\n",
    "y = \\alpha + b_M (MKT-RF) + b_S SMB + b_H HML + \\varepsilon$$\n",
    "\n",
    "- FF5:\n",
    "$$\n",
    "y = \\alpha + b_M (MKT-RF) + b_S SMB + b_H HML + b_R RMW + b_C CMA + \\varepsilon$$\n",
    "\n",
    "- FF5+MOM（MOM取得時のみ）:\n",
    "$$\n",
    "y = \\alpha + b_M (MKT-RF) + b_S SMB + b_H HML + b_R RMW + b_C CMA + b_{Mo} MOM + \\varepsilon$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13-a) Download and parse FF factors from Ken French Data Library\n",
    "\n",
    "import io\n",
    "import re\n",
    "import zipfile\n",
    "import urllib.request\n",
    "from urllib.error import HTTPError, URLError\n",
    "\n",
    "FF_DATA_DIR = ROOT / \"data\"\n",
    "FF_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "FF_SPECS = {\n",
    "    \"ff3\": {\n",
    "        \"output\": FF_DATA_DIR / \"ff_factors_ff3_monthly.csv\",\n",
    "        \"urls\": [\n",
    "            \"https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Research_Data_Factors_CSV.zip\",\n",
    "            \"https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Research_Data_Factors_TXT.zip\",\n",
    "        ],\n",
    "        \"expected_cols\": [\"MKT_RF\", \"SMB\", \"HML\", \"RF\"],\n",
    "    },\n",
    "    \"ff5\": {\n",
    "        \"output\": FF_DATA_DIR / \"ff_factors_ff5_monthly.csv\",\n",
    "        \"urls\": [\n",
    "            \"https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Research_Data_5_Factors_2x3_CSV.zip\",\n",
    "            \"https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Research_Data_5_Factors_2x3_TXT.zip\",\n",
    "        ],\n",
    "        \"expected_cols\": [\"MKT_RF\", \"SMB\", \"HML\", \"RMW\", \"CMA\", \"RF\"],\n",
    "    },\n",
    "    \"mom\": {\n",
    "        \"output\": FF_DATA_DIR / \"ff_mom_monthly.csv\",\n",
    "        \"urls\": [\n",
    "            \"https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Momentum_Factor_CSV.zip\",\n",
    "            \"https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Momentum_Factor_TXT.zip\",\n",
    "        ],\n",
    "        \"expected_cols\": [\"MOM\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def _extract_text_from_zip(content: bytes) -> str:\n",
    "    with zipfile.ZipFile(io.BytesIO(content)) as zf:\n",
    "        members = [n for n in zf.namelist() if n.lower().endswith((\".csv\", \".txt\"))]\n",
    "        if not members:\n",
    "            members = zf.namelist()\n",
    "        if not members:\n",
    "            raise ValueError(\"zip contains no files\")\n",
    "        raw = zf.read(members[0])\n",
    "\n",
    "    for enc in (\"utf-8\", \"latin1\"):\n",
    "        try:\n",
    "            return raw.decode(enc)\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    return raw.decode(\"latin1\", errors=\"ignore\")\n",
    "\n",
    "\n",
    "def _parse_monthly_ff_table(raw_text: str, expected_cols: list[str]) -> pd.DataFrame:\n",
    "    lines = raw_text.replace(\"\\r\", \"\").split(\"\\n\")\n",
    "    pattern = re.compile(r\"^\\s*(\\d{6})\\s*,\")\n",
    "\n",
    "    rows = []\n",
    "    in_block = False\n",
    "\n",
    "    for line in lines:\n",
    "        m = pattern.match(line)\n",
    "        if m:\n",
    "            in_block = True\n",
    "            parts = [p.strip() for p in line.split(\",\")]\n",
    "            if len(parts) < 1 + len(expected_cols):\n",
    "                continue\n",
    "            row = [parts[0]] + parts[1 : 1 + len(expected_cols)]\n",
    "            rows.append(row)\n",
    "        elif in_block:\n",
    "            # Stop at annual section or first non-monthly line after entering block.\n",
    "            break\n",
    "\n",
    "    if not rows:\n",
    "        raise ValueError(\"monthly YYYYMM block not found\")\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"YYYYMM\"] + expected_cols)\n",
    "    df[\"YYYYMM\"] = pd.to_numeric(df[\"YYYYMM\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    df = df.dropna(subset=[\"YYYYMM\"]).copy()\n",
    "\n",
    "    for col in expected_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "        # Ken French missing code often appears near -99.99.\n",
    "        df.loc[df[col] <= -90, col] = np.nan\n",
    "        df[col] = df[col] / 100.0\n",
    "\n",
    "    df[\"date\"] = pd.to_datetime(df[\"YYYYMM\"].astype(int).astype(str), format=\"%Y%m\") + pd.offsets.MonthEnd(0)\n",
    "    df = df.set_index(\"date\")[expected_cols].sort_index()\n",
    "    return df\n",
    "\n",
    "\n",
    "def fetch_ff_dataset(spec_key: str, force_download: bool = False):\n",
    "    spec = FF_SPECS[spec_key]\n",
    "    out_path = spec[\"output\"]\n",
    "\n",
    "    if out_path.exists() and not force_download:\n",
    "        df = pd.read_csv(out_path, index_col=0, parse_dates=True)\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        return df, \"cached\", str(out_path), \"\"\n",
    "\n",
    "    last_error = \"\"\n",
    "    for url in spec[\"urls\"]:\n",
    "        try:\n",
    "            with urllib.request.urlopen(url, timeout=45) as resp:\n",
    "                content = resp.read()\n",
    "            raw_text = _extract_text_from_zip(content)\n",
    "            df = _parse_monthly_ff_table(raw_text, spec[\"expected_cols\"])\n",
    "            df.to_csv(out_path)\n",
    "            return df, \"downloaded\", url, \"\"\n",
    "        except (HTTPError, URLError, TimeoutError, zipfile.BadZipFile, ValueError) as exc:\n",
    "            last_error = f\"{type(exc).__name__}: {exc}\"\n",
    "            print(f\"[WARN] {spec_key} failed from {url} -> {last_error}\")\n",
    "\n",
    "    return None, \"failed\", \"\", last_error\n",
    "\n",
    "\n",
    "ff_results = {}\n",
    "ff_status_rows = []\n",
    "force_ff = bool(CONFIG.get(\"force_ff_download\", False))\n",
    "\n",
    "for key in [\"ff3\", \"ff5\", \"mom\"]:\n",
    "    df, status, source, err = fetch_ff_dataset(key, force_download=force_ff)\n",
    "    ff_results[key] = df\n",
    "    ff_status_rows.append(\n",
    "        {\n",
    "            \"dataset\": key,\n",
    "            \"status\": status,\n",
    "            \"source_or_file\": source,\n",
    "            \"error\": err,\n",
    "            \"n_rows\": (0 if df is None else len(df)),\n",
    "        }\n",
    "    )\n",
    "\n",
    "ff_status_df = pd.DataFrame(ff_status_rows)\n",
    "ff_status_path = OUT_TABLE_DIR / \"ff_download_status.csv\"\n",
    "ff_status_df.to_csv(ff_status_path, index=False)\n",
    "\n",
    "print(ff_status_df)\n",
    "print(\"saved:\", ff_status_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13-b) Monthly returns, FF regressions, and output figures\n",
    "\n",
    "\n",
    "def daily_to_monthly_simple(ret: pd.Series) -> pd.Series:\n",
    "    return (1.0 + ret).resample(\"ME\").prod() - 1.0\n",
    "\n",
    "\n",
    "def fit_ols_with_tstats(y: pd.Series, X: pd.DataFrame, x_cols: list[str]):\n",
    "    merged = pd.concat([y.rename(\"y\"), X[x_cols]], axis=1).dropna()\n",
    "    n = len(merged)\n",
    "    k = len(x_cols)\n",
    "    if n <= (k + 1):\n",
    "        return None\n",
    "\n",
    "    yv = merged[\"y\"].to_numpy(dtype=float)\n",
    "    xv = merged[x_cols].to_numpy(dtype=float)\n",
    "    Xmat = np.column_stack([np.ones(n), xv])\n",
    "\n",
    "    coef, _, _, _ = np.linalg.lstsq(Xmat, yv, rcond=None)\n",
    "    fitted = Xmat @ coef\n",
    "    resid = yv - fitted\n",
    "\n",
    "    sse = float((resid ** 2).sum())\n",
    "    sst = float(((yv - yv.mean()) ** 2).sum())\n",
    "    r2 = (1.0 - sse / sst) if sst > 1e-12 else np.nan\n",
    "\n",
    "    dof = n - (k + 1)\n",
    "    if dof > 0:\n",
    "        sigma2 = sse / dof\n",
    "        xtx_inv = np.linalg.pinv(Xmat.T @ Xmat)\n",
    "        se = np.sqrt(np.diag(sigma2 * xtx_inv))\n",
    "        tvals = np.where(se > 1e-12, coef / se, np.nan)\n",
    "    else:\n",
    "        tvals = np.full_like(coef, np.nan)\n",
    "\n",
    "    res = {\n",
    "        \"nobs\": int(n),\n",
    "        \"r2\": float(r2),\n",
    "        \"alpha_monthly\": float(coef[0]),\n",
    "        \"alpha_annualized\": float((1.0 + coef[0]) ** 12 - 1.0),\n",
    "        \"t_alpha\": float(tvals[0]) if np.isfinite(tvals[0]) else np.nan,\n",
    "    }\n",
    "\n",
    "    for j, name in enumerate(x_cols, start=1):\n",
    "        res[f\"beta_{name}\"] = float(coef[j])\n",
    "        res[f\"t_{name}\"] = float(tvals[j]) if np.isfinite(tvals[j]) else np.nan\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "# Strategy monthly series (net)\n",
    "strategy_monthly = {\n",
    "    mode: daily_to_monthly_simple(results_by_mode[mode][\"bt\"][\"net_ret\"])\n",
    "    for mode in CONFIG[\"neutral_modes\"]\n",
    "}\n",
    "eqw_monthly = daily_to_monthly_simple(eqw_bt[\"net_ret\"])\n",
    "\n",
    "active_monthly = {\n",
    "    f\"{mode}_minus_EQW\": strategy_monthly[mode] - eqw_monthly\n",
    "    for mode in CONFIG[\"neutral_modes\"]\n",
    "}\n",
    "\n",
    "ff3 = ff_results.get(\"ff3\")\n",
    "ff5 = ff_results.get(\"ff5\")\n",
    "mom = ff_results.get(\"mom\")\n",
    "\n",
    "factor_models = {}\n",
    "if ff3 is not None and {\"MKT_RF\", \"SMB\", \"HML\", \"RF\"}.issubset(ff3.columns):\n",
    "    factor_models[\"FF3\"] = {\n",
    "        \"data\": ff3,\n",
    "        \"x_cols\": [\"MKT_RF\", \"SMB\", \"HML\"],\n",
    "        \"has_rf\": True,\n",
    "    }\n",
    "\n",
    "if ff5 is not None and {\"MKT_RF\", \"SMB\", \"HML\", \"RMW\", \"CMA\", \"RF\"}.issubset(ff5.columns):\n",
    "    factor_models[\"FF5\"] = {\n",
    "        \"data\": ff5,\n",
    "        \"x_cols\": [\"MKT_RF\", \"SMB\", \"HML\", \"RMW\", \"CMA\"],\n",
    "        \"has_rf\": True,\n",
    "    }\n",
    "\n",
    "if ff5 is not None and mom is not None:\n",
    "    ff5_mom = ff5.join(mom[[\"MOM\"]], how=\"inner\")\n",
    "    if {\"MKT_RF\", \"SMB\", \"HML\", \"RMW\", \"CMA\", \"MOM\", \"RF\"}.issubset(ff5_mom.columns):\n",
    "        factor_models[\"FF5_MOM\"] = {\n",
    "            \"data\": ff5_mom,\n",
    "            \"x_cols\": [\"MKT_RF\", \"SMB\", \"HML\", \"RMW\", \"CMA\", \"MOM\"],\n",
    "            \"has_rf\": True,\n",
    "        }\n",
    "\n",
    "rows = []\n",
    "\n",
    "for mode in CONFIG[\"neutral_modes\"]:\n",
    "    series = strategy_monthly[mode]\n",
    "    for model_name, model_spec in factor_models.items():\n",
    "        fac = model_spec[\"data\"]\n",
    "        x_cols = model_spec[\"x_cols\"]\n",
    "\n",
    "        if model_spec[\"has_rf\"] and \"RF\" in fac.columns:\n",
    "            y = series - fac[\"RF\"]\n",
    "            return_type = \"strategy_excess\"\n",
    "        else:\n",
    "            y = series.copy()\n",
    "            return_type = \"strategy_raw\"\n",
    "\n",
    "        fit = fit_ols_with_tstats(y=y, X=fac, x_cols=x_cols)\n",
    "        if fit is None:\n",
    "            continue\n",
    "        fit.update(\n",
    "            {\n",
    "                \"strategy_id\": mode,\n",
    "                \"series_kind\": \"strategy_net\",\n",
    "                \"model\": model_name,\n",
    "                \"return_type\": return_type,\n",
    "            }\n",
    "        )\n",
    "        rows.append(fit)\n",
    "\n",
    "for key, series in active_monthly.items():\n",
    "    for model_name, model_spec in factor_models.items():\n",
    "        fac = model_spec[\"data\"]\n",
    "        x_cols = model_spec[\"x_cols\"]\n",
    "\n",
    "        # Active return proxy is treated as self-financing spread (no RF subtraction).\n",
    "        y = series.copy()\n",
    "        fit = fit_ols_with_tstats(y=y, X=fac, x_cols=x_cols)\n",
    "        if fit is None:\n",
    "            continue\n",
    "        fit.update(\n",
    "            {\n",
    "                \"strategy_id\": key,\n",
    "                \"series_kind\": \"active_minus_eqw\",\n",
    "                \"model\": model_name,\n",
    "                \"return_type\": \"active_raw\",\n",
    "            }\n",
    "        )\n",
    "        rows.append(fit)\n",
    "\n",
    "ff_attr_df = pd.DataFrame(rows)\n",
    "ff_attr_path = OUT_TABLE_DIR / \"ff_attribution.csv\"\n",
    "ff_attr_df.to_csv(ff_attr_path, index=False)\n",
    "\n",
    "print(\"factor models:\", list(factor_models.keys()))\n",
    "print(\"saved:\", ff_attr_path)\n",
    "print(ff_attr_df.head(12))\n",
    "\n",
    "# Figure 1: alpha comparison bar\n",
    "alpha_fig_path = OUT_FIG_DIR / \"ff_alpha_bar.png\"\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "if not ff_attr_df.empty:\n",
    "    alpha_plot = ff_attr_df.copy()\n",
    "    alpha_plot[\"label\"] = alpha_plot[\"strategy_id\"] + \"|\" + alpha_plot[\"model\"]\n",
    "    alpha_plot = alpha_plot.sort_values(\"alpha_annualized\", ascending=False)\n",
    "\n",
    "    ax.bar(alpha_plot[\"label\"], alpha_plot[\"alpha_annualized\"] * 100.0)\n",
    "    ax.set_xticks(np.arange(len(alpha_plot)))\n",
    "    ax.set_xticklabels(alpha_plot[\"label\"], rotation=70, ha=\"right\", fontsize=8)\n",
    "    ax.axhline(0.0, color=\"black\", linestyle=\"--\", linewidth=1.0)\n",
    "\n",
    "ax.set_title(\"FF Attribution: Annualized Alpha\")\n",
    "ax.set_ylabel(\"alpha (% p.a.)\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "fig.savefig(alpha_fig_path, dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Figure 2: beta heatmap (priority: FF5_MOM > FF5 > FF3)\n",
    "beta_fig_path = OUT_FIG_DIR / \"ff_betas_heatmap.png\"\n",
    "priority = [\"FF5_MOM\", \"FF5\", \"FF3\"]\n",
    "model_for_heatmap = next((m for m in priority if m in factor_models), None)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4.8))\n",
    "if model_for_heatmap is not None and not ff_attr_df.empty:\n",
    "    factor_cols = factor_models[model_for_heatmap][\"x_cols\"]\n",
    "    sub = ff_attr_df[\n",
    "        (ff_attr_df[\"model\"] == model_for_heatmap)\n",
    "        & (ff_attr_df[\"series_kind\"] == \"strategy_net\")\n",
    "    ].copy()\n",
    "\n",
    "    if not sub.empty:\n",
    "        mat_df = sub.set_index(\"strategy_id\")[[f\"beta_{c}\" for c in factor_cols]]\n",
    "        mat = mat_df.values\n",
    "        vmax = np.nanmax(np.abs(mat)) if np.isfinite(np.nanmax(np.abs(mat))) else 1.0\n",
    "        vmax = max(vmax, 0.2)\n",
    "\n",
    "        im = ax.imshow(mat, aspect=\"auto\", cmap=\"coolwarm\", vmin=-vmax, vmax=vmax)\n",
    "        ax.set_yticks(np.arange(len(mat_df.index)))\n",
    "        ax.set_yticklabels(mat_df.index)\n",
    "        ax.set_xticks(np.arange(len(factor_cols)))\n",
    "        ax.set_xticklabels(factor_cols)\n",
    "        ax.set_title(f\"FF Betas Heatmap ({model_for_heatmap}, strategy_net)\")\n",
    "        fig.colorbar(im, ax=ax, fraction=0.025, pad=0.01, label=\"beta\")\n",
    "\n",
    "ax.grid(False)\n",
    "fig.savefig(beta_fig_path, dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"saved:\", alpha_fig_path)\n",
    "print(\"saved:\", beta_fig_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11) Plot & Save\n",
    "\n",
    "nav_fig_path = OUT_FIG_DIR / \"final_method_nav_none_A_B_vs_eqw.png\"\n",
    "beta_fig_path = OUT_FIG_DIR / \"final_method_rolling_beta_none_A_B.png\"\n",
    "turnover_fig_path = OUT_FIG_DIR / \"final_method_turnover_none_A_B.png\"\n",
    "cost_fig_path = OUT_FIG_DIR / \"final_method_cumulative_cost_none_A_B.png\"\n",
    "\n",
    "# NAV\n",
    "fig, ax = plt.subplots(figsize=(11, 4.5))\n",
    "for mode in CONFIG[\"neutral_modes\"]:\n",
    "    nav = results_by_mode[mode][\"bt\"][\"net_nav\"]\n",
    "    ax.plot(nav.index, nav.values, label=f\"strategy:{mode}\")\n",
    "ax.plot(eqw_bt.index, eqw_bt[\"net_nav\"], linestyle=\"--\", color=\"black\", label=\"benchmark:EQW\")\n",
    "ax.set_title(\"Final Method NAV (net)\")\n",
    "ax.set_ylabel(\"NAV\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(loc=\"best\")\n",
    "fig.savefig(nav_fig_path, dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Rolling beta\n",
    "fig, ax = plt.subplots(figsize=(11, 4))\n",
    "for mode in CONFIG[\"neutral_modes\"]:\n",
    "    rb = results_by_mode[mode][\"rolling_beta\"]\n",
    "    ax.plot(rb.index, rb.values, label=mode)\n",
    "ax.axhline(CONFIG[\"beta_target\"], linestyle=\"--\", color=\"black\", linewidth=1.0, label=\"beta_target\")\n",
    "ax.set_title(\"Rolling Beta vs EQW Factor\")\n",
    "ax.set_ylabel(\"beta\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(loc=\"best\")\n",
    "fig.savefig(beta_fig_path, dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Turnover\n",
    "fig, ax = plt.subplots(figsize=(11, 4))\n",
    "for mode in CONFIG[\"neutral_modes\"]:\n",
    "    t = results_by_mode[mode][\"bt\"][\"turnover\"]\n",
    "    ax.plot(t.index, t.values, label=mode)\n",
    "ax.set_title(\"Daily Turnover\")\n",
    "ax.set_ylabel(\"turnover\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(loc=\"best\")\n",
    "fig.savefig(turnover_fig_path, dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Cumulative cost\n",
    "fig, ax = plt.subplots(figsize=(11, 4))\n",
    "for mode in CONFIG[\"neutral_modes\"]:\n",
    "    c = results_by_mode[mode][\"bt\"][\"cum_cost\"]\n",
    "    ax.plot(c.index, c.values, label=mode)\n",
    "ax.set_title(\"Cumulative Transaction Cost\")\n",
    "ax.set_ylabel(\"cum cost\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(loc=\"best\")\n",
    "fig.savefig(cost_fig_path, dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "log_path = OUT_LOG_DIR / \"final_method_full_log.txt\"\n",
    "log_path.write_text(\"\\n\".join([\n",
    "    str(metrics_train_test_path),\n",
    "    str(mode_summary_path),\n",
    "    str(eqw_summary_path),\n",
    "    str(nav_fig_path),\n",
    "    str(beta_fig_path),\n",
    "    str(turnover_fig_path),\n",
    "    str(cost_fig_path),\n",
    "]))\n",
    "\n",
    "print(\"saved figures:\")\n",
    "print(nav_fig_path)\n",
    "print(beta_fig_path)\n",
    "print(turnover_fig_path)\n",
    "print(cost_fig_path)\n",
    "print(\"saved log:\", log_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (10) Interpretation Notes（実務解釈）\n",
    "\n",
    "- 超過収益の源泉は、テーマ間の相対的なモメンタム加速を月次で取りに行くローテーション効果です。\n",
    "- beta-neutral（A/B）は、$r^{EQW}$ に共通する成分を削る方向に働き、テーマ選抜の純粋成分を観測しやすくします。\n",
    "- `none` は共通成分も保持するため、上昇局面ではリターン水準が高くなりやすい一方、EQW感応度も残ります。\n",
    "- 失敗しやすい条件は、(i) コスト上昇、(ii) turnover急増、(iii) テーマ間相関上昇（分散低下）です。\n",
    "- **最終採択モードは `none`**（`docs/notes/70_final_spec.md` 準拠）です。A/B は検証比較用オーバーレイとして扱います。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}